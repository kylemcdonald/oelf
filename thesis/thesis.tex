\documentclass{thesis}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}

\thesistitle{\bf Only Everything Lasts Forever}        
\author{Kyle McDonald}        
\degree{Master of Fine Arts}
\department{Electronic Arts}
\projadviser{Curtis Bahn}
\coprojadviser{Michael Century}
\cocoprojadviser{Shawn Lawson}
\submitdate{April 2010\\(For Graduation May 2010)}
\copyrightyear{2010}
      
\begin{document} 
\titlepage
\tableofcontents
%\listoftables % required if there are tables
%\listoffigures % required if there are figures

\specialhead{Acknowledgments}

(Insert acknowledgments here.)

\specialhead{Abstract}

Only Everything Lasts Forever is a sound composition containing every sound we can distinguish. It explores the social and political associations of sound representation along with the psychology and philosophy of noise and emptiness.

\chapter{Introduction}

Noise is an invention. Not only an invention, but a necessity. Noise is misunderstanding and non-contextuality. As long as there are observers, there will be noise---and without observers, there is no noise.

Historically, noise has had a number of definitions. A close definition to mine is ``that outside of a given representation'', given by Doug Van Nort in ``Noise/music and representation systems''\cite{Vannort06}. Or consider the definitions of Claude Shannon, Luigi Russolo\cite{Russolo04}, John Cage\cite{Cage61}, Jacques Attali\cite{Attali85}, Kim Cascone\cite{Cascone00}, Douglas Kahn\cite{Kahn01} and Paul Hegarty\cite{Hegarty02}.

I propose an alternative definition of noise built on the notion of \emph{non-contextuality}. Rather than placing noise in the usual opposition to music, I will explore this definition directly. First, I will present a number of examples that outline the sense of ``context'' I'm using. Then I will discuss art works focused on collaborating with and revealing these contexts. Finally, I will describe my own work, ``Only Everything Lasts Forever'', and its exploration of the context of the MP3 format.

\chapter{Understanding Context}

\section{Metaphor, Coherence and Interdependence}

In ``Metaphors We Live By'', George Lakoff and Mark Jonhnson explore the thesis that our beliefs and actions are structured by a metaphorical ``web'' of understanding. The word ``context'' is itself rooted in a metaphor: the Latin ``contexere'' describes the ``joining together'' of weaving. In this paper, the word ``context'' refers to this idea of a system that is supported by intertwined relationships rather than a single foundation. These relationships may be self-referential, as Douglas Hofstadter argues in the case of cognition\cite{Hofstadter01}\footnote{Hofstadter even continues, describing not only cognition as analogical, but consciousness itself as fundamentally self-referential\cite{Hofstadter07}.}. In accordance with metaphor as the foundation for understanding: when we have no context for a sound, we cannot conceptualize it so we call it noise.

In Western philosophy, context manifests itself as the coherence theory of truth\cite{Blackburn07}\cite{young_coherence_????}, where truth is identified by its coherence with other propositions. This theory stands in opposition to the correspondence\cite{david_correspondence_????} theory of truth, where truth is identified by correspondence to an external system. In accordance with coherence as the foundation for belief: when we have no context for a sound, we cannot identify its meaning, so we call it noise.

In Eastern philosophy, context manifests itself in the Buddhist doctrine of \emph{pratityasamutpada}, or ``interdependent arising''\footnote{Cage uses the translation ``interpenetration'' to describe this concept.}, which accounts for the causal relationship between all things.\cite{Koller01} This is deeply tied to the notion of \emph{sunyata}, or ``emptiness'', which asserts that nothing has an inherent essence but exists only in relation to everything else. In accordance with interdependence and emptiness as metaphysical truths: when we have no context for a sound, there is no essence to be found, so we call it noise.

\section{A Signal Appears as Noise}

\begin{quote}
Take a photograph of your head inside a freezer. Upload this photo to the internet (like flickr). Tag the file with 241543903. The idea is that if you search for this cryptic tag, all the photos of heads in freezers will appear. I just did one.
\end{quote}

Sometimes the deeper context of apparent noise is later revealed. The above instructions were sent by David Horvitz to a mailing list in 2009\footnote{\url{http://davidhorvitz.com/2009/}}. To a casual surfer, this unusual photo paired with a ``cryptic'' number\cite{david_horvitz_flickr:_????-1} would appear as noise. But once the context of Horvitz' instructions are revealed, the image and tag are no longer understood as noise. Alternatively, once enough photos are surveyed with the same theme and tag, the context becomes the set of images.\footnote{Also see ``MOOKDFJLAL''\cite{david_horvitz_flickr:_????}, where Horvitz instructs people to wear pillow cases over their heads.}

In the case of a tagged image, you might suspect a deper context than the superficially obvious one. In computer security, apparently indecipherable noise can sometimes be reverse engineered to reveal a deeper context where none was originally even suspected---a context beyond intuition, but within the reach of computation. These ``side-channel attacks'' might consist of simply zooming in on a reflection of a screen at a distance to snoop on the displayed information,\cite{w._wayt_gibbs_hackers_2009} or recording the emitted RF radiation of a CRT monitor and amplifying it for display on another screen.\cite{erik_thiele_tempest_????} Variations in a computer's sluggishness can reveal the contents of its computation (called a timing attack). The LED on a router may appear to blink at random, but they are sometimes driven directly by the data line---and can therefore be optically recorded and decoded.\footnote{Similar computer security techniques were infamously applied to daily life, as ``reality-cracking''\cite{francesco_vianello_reality_????}, by Fravia (Francesco Vianello)---a prominent software cracker who regularly published various technical details of his practice along with manifestos on cracking philosophy. He proposed: the same way line after line of assembly code may at first appear impenetrable and unrelated to the program it represents, the chaos of daily life may seem irrelevant to our belief systems and practices and those of others around us. But they are related, and it only takes time to understand the ``concealed truths''. Fravia was particularly focused on ``seeing through'' consumerism, but was interested in everything---including body language, food additives, war propaganda, telemarketing, and bottled water.}

All of these cases point to the possibility of missing context: hearing sounds as ``noise''.

\section{Noise Appears as a Signal}

Sometimes we filter noise with an inapplicable preconceived context, revealing a signal that only temporarily retains coherence. We have an especially strong bias for anthropomorphization: astronomical features like the face on Mars\cite{brian_dunning_facemars_2008} and the CfA2 Great Wall ``stick man''\cite{de_lapparent_slice_1986}, religious figures like Jesus, Mary and Saraswati in stains and foods\cite{boston.com_religious_????}\footnote{Dan Paluska satirizes these sightings with his ``Holy Toaster''\cite{dan_paluska_holy_2005}: an art object and commercially available toaster insert that burns the face of Jesus onto every piece of toast inserted.} and Ganesha in mathematical visualizations\cite{melinda_green_buddhabrot_1993}, or the ghosts of electronic voice phenomena\footnote{In ``Ghosts in the Machine'' by Alan Dunning, Paul Woodrow, and Dr. Morley Hollenberg\cite{alan_dunning_paul_woodrow_and_morley_hollenberg_einsteins_2008}, a camera is placed inside a black box, with the image formed on the sensor amplified in software and automatic face detection applied. The computer then projects the image from the camera with a rectangle around any region identified as containing a ``face''.}.

Gestalt theory describes some of these phenomena in terms of pattern recognition, covering cases of preconceived context like the Kanizsa triangle illusion\cite{alexander_bogomolny_kanizsa_????} and acquired context as in the case of the dalmatian illusion\cite{michael_bach_dalmatian_2002}. Other systems are more dependent on intellectual reflection: the Bible Code\footnote{Originally ``Equidistant Letter Sequences in the Book of Genesis''\cite{rips_equidistant_1994}.} and Jonathan Feinberg's ``Haiku Finder'', the Voynich manuscript\cite{robin_mckie_secret_2004}, the Golden Ratio\cite{Doczi81}\cite{markowsky_misconceptions_1992} and Le Corbusier's ``Modulor''\cite{padovan_proportion_1999}, prime spirals\cite{michael_m._ross_natural_2007}\cite{weisstein_prime_????}, or the SETI@home project\cite{seti_about_????}\footnote{SETI@home is a particularly good example of divining a signal from noise. First they defined their context for an ``intelligent sign'', based on the notion of narrow band repetition from fixed sources. Then they constructed a complex hardware and software network for analyzing signals in terms of that context. SETI@home experiences regular false alarms from pulsars, including the officially unidentified 1420 MHz radio source SHGb02+14a\cite{eugenie_samuel_reich_mysterious_2004}, which fits this context by definition but not in spirit. Something akin to the jackhammer outside that can be called meaningful but not musical.}. Each of these cases rely on intense analysis of a noisy system until a coherence emerges. The more general coherence of any of these systems can be analyzed using standard critical thinking techniques\cite{Moore07}---for example, prime spirals cohere with mathematics better than the Bible Code does with statistics.

All of these cases point to the possibility of misunderstanding a context as ``universal'' or, at least, biologically relevant: hearing sounds as ``music'' when they might be better classified as non-musical non-noises.

\chapter{Revealing Implicit Contexts}

\section{Empty Art}

	\begin{quote}
	The Faithful who gather at the mosque of Amr, in Cairo, are acquainted with the fact that the entire universe lies inside one of the stone pillars that ring its central court... No one, of course, can actually see it, but those who lay an ear against the surface tell that after some short while they perceive its busy hum...
	\end{quote}

Part of the context implicit to a medium is the space of possible work within that medium. This space can be compared to the titular subject of Jorge Luis Borges' short story ``The Aleph''\cite{borges_aleph_2004}. While the Aleph in the story is a visual apparition, the author reveals the possibility of a sonic corollary in the above postscript. The Aleph is described as ``the only place on earth where all places are---seen from every angle, each standing clear, without any confusion or blending''.\footnote{This unity is found even in its name. Kabbalah sees the letter as ``En Soph, the pure and boundless godhead'', and provides a number of intricate numerological interpretations. \url{http://gnosticteachings.org/courses/alphabet-of-kabbalah/01-aleph} The aleph is seen as representing the Kabbalistic Holy Trinity, and this ``explains'' the facts that: $\aleph$ represents the number 1, or unity, and appears at the beginning of the Hebrew aleph-bet; the spelling of its name consists of three characters---the Trinity; and the values of these characters---80, 30, 1---sum to 111, or three alephs. The aleph is also associated with the breath of life---and in this way it mirrors the ``Aum'' of Hinduism that may also represent breath, unity, and the creative force of the universe.} The Aleph of a given medium can be represented in spirit through an Empty work.

Some seminal Empty works include Marcel Duchamp's ready-mades, 4'33'' by John Cage\cite{larry_j_solomon_sounds_1998}, Robert Rauschenberg's white paintings, Kazimir Malevich's early Suprematist work\cite{moma_kazimir_2006}, Alexander Rodchenko's ``pure'' colors\cite{moma_rodchenko_1998}, and Peter Ablinger's work with white noise\footnote{It's worth mentioning the Dadaist ``In Futurum'' by   Erwin Schulhoff, with its complex selection of rhythmic silence; as well as Alphonse Allais' various monochromes and 24 empty measures of ``Funeral March''. The modern equivalent to these works might be Cory Arcangel's Photoshop gradients.\cite{cory_arcangel_photoshop_2009}}. Each of these works appeared in unique historical and aesthetic environments, but they share the common feature of either accepting or representing everything.

While Duchamp accepted all objects as potential sculptural material, Cage accepted all sounds as potential musical material, saying ``Music is continuous. It is only we who turn away''.\footnote{I see Cage's equivocation of attended sound with music as a poetic extrapolation of his Zen influences. Attended sound has context and therefore meaning, and it is no longer noise (or ``silence''), but it is not necessarily music. When Cage says ``Try as we may to make a silence, we cannot, one need not fear for the future of music'', the irony echoes our intuitive distinction between non-noise and music.}

A performance Cage's silent piece bears the most resemblance to Rauschenberg's ``airports for shadows'', but the score can be compared to the work of Malevich and Rodchenko. In the same way 4'33'' contains only three movements of empty measures, Malevich and Rodchenko presented canvases lacking any subject beyond the materials, construction, and arrangement of the work. Malevich reduces painting to its ``essence''---here, the space of painting:

	\begin{quote}
	For art is the ability to construct, not on the interrelation of form and color, and not on an aesthetic basis of beauty in composition, but on the basis of weight speed and the direction of movement. [...] Color and texture in painting are ends in themselves. They are the essence of painting, but this essence has always been destroyed by the subject.
	\end{quote}
	
And Rodchenko aims for a similar reduction, focusing on the three primary colors of pigment-based subtractive color theory with his own square canvases\footnote{Both Rochenko and Malevich's canvases happen to be square. And the first Suprematist works of Malevich (``Black Square'', ``Red Square'', ``White on White'') have only squares as their ``subject''---which he called a ``zero form''. More geometrically elegant is the circle, more intuitive to paint is the line, or even the point. But the square is a sort of non-form within the structural constraints of the canvas. This is echoed in the present day by our rectangular pixel-based display surfaces---though the constraint now comes from electrical engineering rather than woodworking.}.

Ablinger offers a sonic corollary to these monochromes, presenting only white noise with no subject beyond the configuration of the speakers themselves:

	\begin{quote}
	Rauschen (White noise) is the totality of sounds---``everything always'' in its acoustic representation. Comparable to white light that contains all colours, white noise contains all frequencies, and---poetically speaking---all music. [...] The reason why we hear ``less than nothing'' is that we cannot connect to it by just listening. It is simply too much. We can't do anything with it. The only thing that is left to do is to produce illusions, i.e., to hear something ``in'' the noise that is not there, that can be perceived only individually---to project our own imagination onto that white ``screen''. In this way Rauschen works like a mirror, reflecting back only what we project onto it.\footnote{\url{http://ablinger.mur.at/rauschen.html}}
	\end{quote}
	
Our appreciation for this sonic space comes down to our ability, as humans, to ``to think of simultaneous things as successive ones''\cite{christian_scheib_statics_????}---misunderstanding interdependence as independence, hearing noise. In another example of this same principle, Ablinger writes ``redundancy produces information'', and Christian Scheib adds ``there is necessarily always something changing [...] repetition does not exist except as an abstraction''. In hearing only redundancy, we miss inherent variation and interdependent context.
	
\section{Glitch Art}

RYbN's monochrome performances\footnote{\url{http://www.cimatics.com/festival2008/festival/performances/aka_pfmonochrome.html}} involve collecting and editing together various ``black'' sequences from online videos. When played in complete darkness, a variety of artifacts emerge due to the nature of lossy video compression. This work is a bridge between more traditional Empty art surveyed above and a newer technique for revealing the implicit context of a medium: Glitch art. The foundation of Glitch is the simple principle that minor changes in the space of the encoded information cause major changes in the perceptual space of the receiver.

Glitch art as a method for exploring the context of format might be compared to musical improvisation in an unusual space. In the same way every sonic gesture collaborates with the otherwise ``silent'' acoustic context, each glitch corrupts the natural content of a file in order to reveal its structure.\footnote{Another metaphorical interpretation of Glitch comes from cognitive psychology: V.S. Ramachandran espouses the notion of looking at where things break to understand how they work\cite{ramachandran_phantoms_1999}. When multiple patients exhibit similar disabilities from lesions in a similar area of the brain, the affected region is assumed responsible for the modified behavior. This technique can also be applied to normal operation: the idea of a malleable body-image and its ``phantom limb'' glitch, the notion of ``filling-in'' happening regularly in our blindspots at the optic nerve (or sometimes more extremely in the case of cataracts and blindsight), etc.}

While glitches may occur in uncompressed formats\footnote{Which was often the case with the Yahoo databenders group\cite{indianropeburn_databenders_????}, as uncompressed or ``raw'' formats have few syntactic requirements apart from short headers and therefore can be transcoded almost directly without any modifications to the data.}, they generally have more significant effects when they are compressed. The two types of compression---lossy and lossless---offer very different approaches to data encoding. Lossless compression algorithms are purely syntactic, but lossy compression considers the signal's semantics from the perspective of the receiver. Lossless compression assumes only that a medium can be digitized, but lossy compression goes one step further and defines an associated context for the medium. In theory this context is directly driven by ``universals'' of human perception, but in practice it is standardized by small groups of specialists. Lossy compression might be compared to the shorthands used by ethnographers and composers, while lossless compression is more like magnetic tape in that it treats sounds just like any other time-varying signal.

Yasunao Tone's compositions and performances for ``wounded CD''\cite{media_art_net_media_2010} provide a simple insight into the spiral nature of music CDs through repetitive 120 ms loops. While the sonic content of work like ``Wounded Man'yo'' is derived from an equally complex system, the repetition imposed by the CD player is what provides the structure that allows us to connect with the sound. Not just in a rhythmic sense, but through our nostalgia for failure---something also present in the music of the 20 kbps label\footnote{\url{http://20kbps.sofapause.ch/}}. This affective aspect of Glitch is confirmed by more than five thousand submissions to the Glitch Art Pool on Flickr\cite{liminalmike_flickr:glitch_????}---many submitted by individuals who, in the moment of a system's failure, recognize the glitch as beautiful and desire to share it.
	
A recently popularized glitch is the ``pixel bleed''\cite{john_michael_boling_rhizome_????} effect, known more technically as ``motion artifacts'' and colloquially as ``datamoshing''. This technique reveals the context of inter-frame motion analysis present in all modern lossy video compression algorithm---the assumption that we are significantly more attuned to movement between frames than variation in texture and color\footnote{Our ability to objectify motion of static noise but not noise itself was explored in the non-Glitch piece by Jochem van der Spek, ``No Noise''\cite{jochem_van_der_spek_no_2001}. My own variation on this theme \url{http://openprocessing.org/visuals/?visualID=7072} plays with the notion of an impermanent identity in noisy field.}.

``Download Finished'' by Mediengruppe Bitnik and Sven K\"onig\cite{!mediengruppe_bitnik_and_sven_knig_download_????}  makes heavy use of pixel bleed. In their discussion of the work, they go beyond the context of the format and into the distribution and encoding system surrounding it:

	\begin{quote}
	A film found in a filesharing network is the sum of $1$ the original film, $2$ the work of the mathematicians who laid the theoretical foundations for $3$ the programmers who designed the encoding software / the codec and $4$ the file sharer who finally uses all that software to intentionally make the $5$ film widely available. The processes behind $2-4$ usually stay invisible, leading to the wrong assumption that $1=5$. DOWNLOAD FINISHED transformes [sic] $5$ such, that the processes behind steps $2-4$ become visibile [sic] and show that films found in file sharing networks are actually collaborative works.
	\end{quote}
	
While the aesthetic of ``Download Finished'' celebrates the perceptual chaos that emerges from their ``transmogrifications'', ``The Ceibas Cycle'' by Evan Meaney\cite{evan_meaney_ceibas:_2008} takes a more clinical approach to Glitch. Digital video is seen as a container for a memory that will steadily fade over time, a steady corruption induced artificially by injection of plain text into the bitstream of 27 different codec/container combinations. The resulting videos reveal each format's otherwise invisible encoding and biases about perception: some display pixel bleed artifacts, others quickly strobe magenta when color information goes missing. A survey of the causes behind each effect are rarely elaborated in prose by Glitch artists\footnote{With some exceptions notable exceptions being Cory Arcangel's ``In Compression'' and the writings of Rosa Menkeman.}, and are instead surveyed by research institutions working with signal processing\cite{nikolai_trunichkin_and_dr._dmitriy_vatolin_crazy_????}.

``Only Everything Lasts Forever'' draws on these Glitch works in its exegesis of MP3. Despite the ubiquity of the format, there is surprisingly little in the way of MP3 Glitch art. One example is ``MPeg Fucker'' by imbecil\cite{imbecil_mpeg_2004}, which performs a similar injection as ``The Ceibas Cycle'', but on MP3s using search-and-replace. This search-and-replace technique was also used Mats \"Oljare, one of the original members of the databenders group. \"Oljare is an exception to the other databenders that avoided MP3 due to the difficulty of constructing syntactically valid frames. He overcame the limitations of search-and-replace by using audio editors to edit the raw MP3 bitstream as a whole, using the resulting sonic material\footnote{\"Oljare describes the glitches as ``mostly random short bleeps in the original signal, only sometimes a strange spectral distorsion [sic] of the signal, and sometimes some other peculiar effects like jumps''.} for further composition.

\section{Enumeration and Permutation}
	
Besides presenting an empty space, or using glitches to ``sound'' a space, another technique for revealing the context of a medium is enumeration of the possible contents. While Borges' Aleph presented everything at once, it is ``without any confusion or blending''\footnote{Allowing the story's poet, Carlos Argentino Daneri, to produce his epic work ``The Earth'', which he likens to scripture with all its ``enumeration, congeries, and conglomeration''.'}. The linearization of this massively interwoven space is explored in his earlier work ``The Library of Babel''\cite{borges_library_2000}.
	
In ``The Library of Babel'', the author describes the unusual universe in which he lives: floor after floor of interconnected hexagonal rooms, lined with rows of books on shelves. These shelves are said to contain all the possible 410-page texts, at 40 lines per page, 80 characters per line, and an alphabet of 25 characters. The implication is that these books include:

	\begin{quote}
	Everything: the minutely detailed history of the future, the archangels' autobiographies, the faithful catalogs of the Library, thousands and thousands of false catalogs, the demonstration of the fallacy of those catalogs, the demonstration of the fallacy of the true catalog, the Gnostic gospel of Basilides, the commentary on that gospel, the commentary on the commentary on that gospel, the true story of your death, the translation of every book in all languages, the interpolations of every book in all books.\footnote{In a 2001 post to alt.math.recreational, Keith F. Lynch elaborates further on what exactly is contained in ``everything''.\cite{keith_f._lynch_converting_????} He offers a similarly meandering list as a warning to the original poster who asks about representing $\pi$ in binary, informing them they will be ``guilty of: Copyright infringement, Trademark infringement, Possession of child pornography, Espionage (unauthorized possession of top secret information) [...] Also, your computer will contain all of the nastiest known computer viruses''.}
	\end{quote}

Responding to critics who claim that the library can only ``affirm, negate and confuse everything like a delirious divinity'', the author writes:
	
	\begin{quote}
	In truth, the Library includes all verbal structures, all variations permitted by the twenty-five orthographical symbols, but not a single example of absolute nonsense. [...] I cannot combine some characters, ``dhcmrlchtdj'', which the divine Library has not foreseen and which in one of its secret tongues do not contain a terrible meaning. [...] The Library is unlimited and cyclical. If an eternal traveler were to cross it in any direction, after centuries he would see that the same volumes were repeated in the same disorder (which, thus repeated, would be an order: the Order).
	\end{quote}
	
No book is nonsense precisely because it exists in the context of an alphabet, in the context of a book, and within the Order of the Library itself. Furthermore, from this self-contextualization through enumeration emerges the realization that every possible language is represented---adding another degree of context.
	
This barrage of content, a ``continuous torrent of sensory data'', can be found in the late systems artists\cite{Wright09}, and more recently in Allan McCollum's ``Shapes Project''\cite{allan_mccollum_shapes_2006}\footnote{McCollum is creating one ``shape'' (a solid black object against a white background) for every person on the planet. Rather than design a program to generate these shapes, McCollum is creating them by hand. To make sure he never repeats the same shape twice, he's constructed a complex shape-generating system that he follows rigorously.} Rather than simply presenting an impression of enumeration, a number of more recent works have endeavored to actually enumerate the space within a given medium.

\subsection{One Dimensional Spaces}

The simplest method of enumerating a space with $n$ possible values is to first create a mapping between the natural numbers and those $n$ values, and then count up one at a time. While binary is no better suited to the task of representing natural numbers than decimal, many recent enumerative works draw in binary due to its deep connection to digital technology. For example, Michael Aschauer's ``8-bit''\cite{michael_aschauer_8-bit_????} consists of 8 fluorescent tubes steadily enumerating all 256 configurations, and Sebastian Tomczak\cite{tomczak_hardware-based_2009}\cite{tomczak_all_2009} has explored the various binary enumerations of 32-sample PCM encoded sounds as represented in binary. The content of these works exist in one dimension: a linear signal varying in space, as with ``8-bit'', or time  as in Tomczak's work.
	
Working with the twelve notes of an octave rather than 32 samples of a PCM buffer, Tom Johnson's ``The Chord Catalogue''\cite{tom_johnson_liner_1999} performs a similar iteration through possible sounds: the 8178 possible chords in an octave. While a chord can be represented linearly, like the two above examples, ``The Chord Catalogue'' uses a significantly different enumerative method: first all two notes intervals, then three note chords, then four notes, etc. The lack of reliance on standard binary enumeration might be explained by the fact that music has its own order that significantly precedes binary\footnote{The linear notes specifically mention experiments by Mersenne, saying he ``posed questions in the Livre du chant of his Harmonie universelle as to the number of possible melodies one could construct by changing the order of notes in a 22-note sequence. After much calculation he concluded that there was no point in actually carrying out this experiment, since one could never hear that many melodies in one lifetime.''}.

\subsection{Two Dimensional Spaces}

While sound and text are best understood in a one dimensional space, images are best represented in two. But even this two dimensional space must be unwrapped into a linear bitstream to be stored digitally. As such, the same binary enumeration technique may be used for generating every possible image of a given size and resolution.
	
Perhaps the best known implementation of this idea is John F. Simon Jr.'s ``Every Icon''\cite{john_f._simon_jr._every_????}. He uses a 32x32 grid of black and white pixels, and enumerates through the possibilities in binary. The first row alone has $2^{32}$, or about 4.3 billion, combinations. At 100 frames per second, this takes a little over 16 months to complete. The entire image has $2^{32*32}=2^{1024}$ possibilities, or about $10^{308}$. This will take on the order of $10^{298}$ years to complete. This number is so incredibly large, it's just shy of the largest ``dictionary number'', a ``centillion'', valued at $10^{303}$. In interviews, Simon calls this number ``several hundred trillion''\cite{matthew_mirapaul_in_1997}:
	
	\begin{quote}
	Because there's no word for that amount of time and no word for that large a number, several hundred trillion years is my way of making you think about a very, very long time.
	\end{quote}
	
A random image presented by ``Every Icon'' would appear as indecipherable noise: a reminder that the usual ``raw'' raster image format is a logically derived context rather than a perceptually biased one. If we didn't care about watching ``Every Icon'', and just wanted the computer to consider every possible variation without displaying them, it would still take around $10^{290}$ years. But it takes an observer to either recognize the image or misunderstand it. Echoing the many languages of Borges' library, Simon says:
	
	\begin{quote}
	We could be looking at something that has meaning, but because of who we are and what we are now, we might not recognize it.
	\end{quote}
	
While ``Every Icon'' may be the best known image enumeration piece, there are at least six other similar works worth mentioning. Created the same year as ``Every Icon'' are Jim Campbell's ``The End''\cite{jim_campbell_end_1996} and Sintron's ``God's Eye''\cite{sintron_gods_2003}. While ``Every Icon'' places black pixels against a white background, drawing on Simon's paper-based background, these two other works both use a black background punctuated by colored pixels: a nod to the ``false'', ``nothing'', ``zero'',  or ``the absence of electricity'' that directly implies ``the absence of light'' in the digital domain.
	
The title of Jim Campbell's work echoes a recurring undercurrent to many of these meta-art pieces that attempt to reveal their own context. Allais' ``Funeral March'', Rodchenko's exhibition, ``The Death of Painting'', Campbell's ``The End''. These pieces all refer to some kind of ending or death. On the other hand, Cage reminds us ``one need not fear for the future of music'', and Simon ``wanted to show that even in a simple 32-by-32 space, the possibilities for imaging were vast''\cite{matthew_mirapaul_in_1997}. There is a similar distinction between the angst-filled existential ``void'' described by Sartre\footnote{``Being and Nothingness''} on one hand, and the life-affirming emptiness of Buddhism on the other. The acceptance of all things---through an empty work or an enumerative one---puts the audience in the position of the artist and removes the \"ubermensch otherwise providing a foundation for the individual's aesthetic judgments. Cage seemed to ignore this distinction between the existential and Buddhist perspectives on emptiness:
	
\begin{quote}
Many people in our society now go around the streets and in the buses and so forth playing radios with earphones on and they don't hear the world around them. They hear only what they have chosen to hear. I can't understand why they cut themselves off from that rich experience which is free.
\end{quote}

Sintron, on the other hand, seems to understand, criticizing ``Every Icon'' for making the same mistake\cite{olga_goriunova_and_alexei_shulgin_touching_2003}:

	\begin{quote}
	The EI project is a mini version of God's Eye. The EI project was somewhat shortsighted in scope and did not convey the metaphysical aspects of God's Eye but at the same time attracted a larger audience. EI was a pop piece even though it could have been much more. Even with its exposure, I do not think the mass public paid much attention to it beyond a small circle of techno savvy academic individuals. The mass public is not currently capable of understanding these kinds of works. God's Eye was something I felt needed to be out even if misunderstood and unappreciated at the time.
	\end{quote}
	
While ``Every Icon'' is highly conceptual and sold in editions rather than taking a single form, ``God's Eye'' is closely tied to a physical installation involving a pedestal for the computer screen in front of a blue velvet couch, surrounded by two prints that are completely black and completely white, with a small amorphous chrome sculpture sitting on the floor.
		
Three later enumerative pieces are ``ImageN'' by Leander Seige\cite{leander_seige_imagen_????}, ``Every Possible Picture Machine'' by Carter Burwell\cite{carter_burwell_every_????} and ``magic mirror'' by a user named rom\_min\cite{leonardo_solaas_magic_????} (which was submitted to a project organized by Leonardo Solaas).
	
``ImageN'' shares traits with both ``Every Icon'' and ``The End'': it starts with a black background, is meant for the web, and has a low resolution (only up to 150x150). Like ``God's Eye'', it computes images in non-real-time at 20 million per second. A unique characteristic is that it allows the viewer to describe some parameters of the system. Leander offers a poetic reflection on the project: ``computability is in opposition to the complexity'', affirming that it is very unlikely you will see anything recognizable, again reminding us that the ``raw'' formats are generally unrelated to perception.
	
``Every Possible Picture Machine'' runs at 64x64 pixels using 2-bit grayscale pixels against a black background. Burwell summarizes this whole class of pieces well:
	
	\begin{quote}
	What makes this interesting is that among those pictures will be those of all your ancestors and descendents [sic], the first words of every book that will ever be written. The true digital face of God. What makes it less interesting is that it will take a very long time to display all those pictures.
	\end{quote}
	
``magic mirror'' was only proposed, never implemented. It runs at 720x576 and 25 frames per second with 24-bit color. The resolution and framerate of ``magic mirror'' describe the standard format for PAL DVDs in the same way Borges references the 400-page novel, or Cage references the four and a half minute Muzak song. Similarly, Sintron uses 800x600 24-bit pixels---the standard desktop configuration at the time---and Simon 32x32 1-bit icons.
	
The earliest enumerative piece is ``De Wensput'' (``The Wishing Well'') by Lars Eijssen and Boele Klopman.\cite{remko_scha_every_2001} Produced in 1991, it precedes ``Every Icon'' by five years. It used a 172x172 pixel grid, and was closely tied to its physical installation: an entire truck bed worth of continuous feed printer paper was driven into the gallery space and fed into the computer at regular intervals. Every time the computer finished rendering a drawing, it would print out the image and move on. Hanging in the gallery space were various representational prints prepared in advance (for example, a portrait of the artists) with a note indicating on what date and time that specific print would appear. It featured an interactive function that allowed people to explore the space while the computer was printing, by recursively zooming in to a time line to select a specific date and time. ``De Wensput'' is unique in that it does not count in binary. Instead, it uses the same enumeration as ``The Chord Catalogue''. According to Eijssen, this enumeration was chosen because it is more accessible than binary. For the same reason, ``De Wensput'' was not presented as a minimal conceptual piece, but with a bit of theater (the truck) and an diverse collection of prints offering an immediate sense of the barrage of ``everything''.

\subsection{Permutation}

While enumeration emerges from combinations of fixed positions with variable values, permutation is based on a fixed set of values with variable positions. A perfect example is the tradition of change ringing---the practice of ringing $n$ bells in their $n!$ possible orders using one of many basic order-swapping algorithms, or ``methods''. Alexander Christiaan Jacob's ``allRGB''\cite{alexander_christiaan_jacob_allrgb_2008} project encourages a similar kind of exploration, inviting visitors to the site to submit a 4096x4096 image that contains every possible 24-bit RGB color. 

A more ancient permutative system is the 64 hexagrams of the I Ching. These hexagrams have been presented in a number of arrangements over time. For example: the classic King Wen sequence, The Mawangdui sequence, the Fu Xi sequence, and the Chu Hsi sequence. The King Wen sequence has many inverted pairs but is otherwise not best understood as a mathematically well-defined system. The Mawangdui sequence is similar to concatenation of the x/y position of a hexagram when arranged in an 8x8 grid and indexed by trigrams. The Fu Xi sequence consists of a simple binary enumeration. The Chu Hsi sequence is similar to ``The Chord Catalogue'' arrangement, but each ``section'' (for example, where there are three yin and three yang, or two yin and four yang) is upside down and backwards\footnote{``The Chord Catalogue'' reads: ``The lowest voice that can rise a half-tone does so and any lower voices descend to their points of departure.'' while the Chu Hsi sequence might be written ``The highest voice that can rise a half-tone does so and any higher voices descend as low as possible without crossing another.''}.

I began exploring permutations not because they are a particularly effective way of revealing the context implicit to a medium, but because any enumerative piece is partially defined by which enumeration it uses. For the 64 hexagrams of the I Ching, there are $64!\approx10^{89}$ permutations---or possible enumerations---of which four are given above. For all the chords in the octave, there are $8178!\approx9.4*10^{28,463}$ possible scores. Though all of the pieces in this section use binary representations of their space, only ``The Chord Catalogue'' and ``De Wensput'' use a nonstandard enumeration.

One other permutation of binary enumeration is Gray code. Gray code has the interesting property of only changing one bit between adjacent codes. A Gray code can be enumerated recursively, or by a simple logical operation on the standard binary enumeration.\footnote{While Gray codes are a fairly recent development, they have also been used to analyze the I Ching hexagrams. ``A Reordering of the Hexagrams of the I Ching'' Victor Mair and Stephen McKenna, ``Philosophy East and West'', Oct. 1979}

Unfortunately all the enumerations discussed here---standard binary, Gray code, ``The Chord Catalogue'' and Chu Hsi's slight variant---have two properties that make it difficult to explore a space as large as MP3. The Hamming distance between nearby values---the number of bits that are different---is very small, and the bits that are different are confined to a small region. Both of these properties are independent of bit length. This is why, out of all the pieces in this section, perhaps only ``8-bit'' is genuinely interesting to watch, even if it is enumerated in binary. 256 variations can be explored on a human scale. The two quietly flickering rows of ``Every Icon'' set against thirty more completely empty rows illuminates nothing more than the text paired with it: ``Given: An icon described by a 32 X 32 grid. Allowed: Any element of the grid to be colored black or white. Shown: Every icon.''
		
\section{Long Pieces}

Summary: A final way of revealing a space is to examine it on a different scale.

[Uses existing material.] Paul Slocum's ``Pi House Generator''\cite{paul_slocum_pi_2007} uses the digits of $\pi$ to generate an infinitely long piece of house music. He claims that there is no repetition, but this is technically impossible as the program is deterministic and runs on a computer with limited resources. Furthermore, $\pi$ itself contains an infinite amount of repetition. It would be more accurate to say that the program generates pseudo-random music informed by the digits of $\pi$, and that it's hard to predict what you'll hear next.

[Uses existing material.] Brian Whitman's ``Eigenradio''\cite{brian_whitman_eigenradio_2005} project listened to multiple radio streams and used principle components analysis to create a single stream that represented all of them simultaneously. Whitman is heavily involved with music analysis, and acknowledges that the only way for a computer to ``understand'' music is by learning its relationship to a large amount of extramusical information:
	
	\begin{quote}
	In the real world, does anyone ever hear music with absolutely no context? Even if you hear something on the radio by accident, you make an assumption about the artist simply because they have music on the radio. You also know the station and the time of day, maybe the song that came after it.
	\end{quote}
	
[Uses existing material.] Leif Inge's ``9 Beet Stretch''\footnote{\url{http://www.harsmedia.com/SoundBlog/Archief/00550.php}} takes Beethoven's ninth symphony, usually 65 to 75 minutes long, and stretches it to 24 hours with pitch correction. This is a simple act of ``slowing down''; the poetics of ``Longplayer'' without the intense compositional act.
	
``Longplayer'' by Jem Finer\cite{jem_finer_longplayer_????} is a thousand year long composition scheduled to end on December 31 2999. Composed as the end of the twentieth century, Longplayer is primarily concerned with time and only secondarily with music. The way it solves the musical ``problem'' of creating a one thousand year long composition is interesting: starting with a single 20'20'' composition for Tibetan singing bowls, six two-minute segments are chosen as source material, these segments are played out of phase, and the phase slowly drifts over time. The slowest drifting phase moves with the composition, only completing its phrase when the entire composition is complete. The fastest changing phase makes a cycle every 3.7 days. Longplayer is regularly performed at Trinity Buoy Wharf in London, and is constantly streaming online.
		
John Cage's ``Organ$^2$/ASLAP'' (As Slow As Possible)\cite{john_cage_as_????} is one of the more infamous performances of a Cage piece. While the first performance lasted only 29 minutes, it is now being performed on a custom built organ at St. Burchardi church in Halberstadt, Germany. Starting with a pause in 2000, it is scheduled to last 639 years, ending in 2640. This composition is likely related to Cage's comment that he was ``trying to find a way to make music that does not depend on time''.
				
\chapter{Only Everything Lasts Forever}

\section{Early Work}

Mention my automated looping system in 2007.

After discovering so many variations on the ``Every Icon'' theme, I proposed a new work, ``Every Every Icon'':
	
	\begin{quote}
	This work will include all of the above works, as well as any other variations that may be dreamed up in the future. It will accomplish this by iterating through every possible resolution, at every possible framerate, for every possible bit depth, in every possible order.\footnote{\url{http://erraticsemaphore.blogspot.com/2009/08/every-every-icon.html}}
	\end{quote}
	
Around the same I also created ``Every Song''\footnote{\url{http://openprocessing.org/visuals/?visualID=1170}}, which iterates through all possible equal-tempered songs using twelve notes in the octave. It does this as quickly as possible, with the reference for the tempo coming from the frequency of the lowest note being played. It uses simple binary enumeration, and proceeds in a manner very similar to ``Every Icon'' with the exception of being infinitely long: after the first chord is enumerated, it follows with all the two-chord sequences, then three, etc. I have never personally seen it get beyond playing an F in the second chord.

``pppd''\cite{kyle_mcdonald_pppd_2009} is an indirect approach to enumeration via a random walk. It draws on computability theory\cite{boolos_computability_2002} and the idea of a Turing-complete system. Turing-completeness refers to the ability of a system to compute anything that can be computed. pppd uses a very simple esoteric programming language known as P'' (``P prime prime'') to accomplish this. While practical programming languages are full of statements like \verb!z = x + y!, \verb!println(z)!, and \verb!for(int i = 0; i < n; i++)!, P'' breaks computing down to its essentials: moving a pointer through memory, changing memory, and looping---requiring only six commands, sometimes written as \verb![, ], +, -, <! and \verb!>!. Because P'' is Turing-complete, any program you could write with a language like C++ or Java could also be accomplished with P'' (albeit in a significantly slower and heavily obfuscated manner). Because any possible program can be represented, this means that any possible digital image or sound can also be written to the program's memory space. pppd generates random code, fitting the minor syntactical requirement of balanced brackets, and then visualizes and sonifies the memory space while running the code. In theory, not only can every image and sound be composed, but every compressed representation as well---along with the decompression algorithm. In practice, pppd produces very regular sequences of flashing colors and glitchy sounds. This acts a revelation of the nature of P'': it's likes to representing high-contrast memory sequences within constrained memory regions. Using another language would generate very different results.
	
My nandhopper project\cite{kyle_mcdonald_nandhopper_2008} also draws on enumeration via directed random walks. The NAND is a binary logical operation, meaning it takes two inputs that are either true or false and returns a result of true or false. The NAND has the unusual property of being able to represent any possible logical operation on any number of inputs. In other words, while you might normally express a logical statement using multiple operators like IF, AND, OR, and NOT, they can all be reduced to statements that only use NAND. Practically, this means that any possible digital sound synthesis circuit can be represented using only NAND gates.
	
I first came across NAND gates when exploring capacitive sensing. NAND gates are useful for constructing feedback loops that vary their frequency with respect to capacitance to ground.\footnote{Using a Schmitt trigger NAND like the 4093 you can use one input as an on-off switch while tying the output to the other input via a resistor. The feedback input is also tied to ground via a capacitor. This is a very basic RC circuit.} When I realized that NANDs could be used to simulate every other chip, I started exploring various NAND configurations and ways of connecting multiple NAND circuits. I developed three different configurations for an experimental filmmaker\footnote{Mexican filmmaker Diego Delmar, alias ``Macushi'' \url{http://www.knock.com.mx/}} based on intuitive explorations of these configurations. I published these designs along with detailed instructions on the tutorial-sharing site Instructables.com. In Spring of 2009 I developed a close connection to a complex variation on the NAND-synth that was developed for a performance context, integrating capacitive, resistive, and illumination sensing in a massively entangled feedback circuit. I see these as the first steps in a more general exploration of NAND-synthesis. I imagine a massive matrix of reconfigurable NAND circuits that can be rearranged during performance: constantly responding the the changing flow of logic, able to simulate any imaginable digital synthesizer in a very practical sense, complex enough to be interesting, but deterministic enough to be learned.
	
Another project, ``Future Fragments'', was an exploration of glitch-collaboration similar to ``The Ceibas Cycle''. I described it as:
	
	\begin{quote}
	An anti-time-capsule: quotes from seven fellow art students, transcribed phonetically and encoded as colors. Prints of these colors were carried by the artists for a summer. Five returned. Two extra prints were accidentally intercepted and also returned. Decoded back into phonemes and re-formed into words, each text offers an indirect account of their respective journeys.\footnote{\url{http://www.flickr.com/photos/kylemcdonald/sets/72157608915887288/}}
	\end{quote}
	
	A significant portion of this piece was the time spent developing a meaningful abstract encoding for phonemes. Just as a lossily encoded movie will yield something perceptually similar to the original even in the face of glitch, I developed a lossless encoding that was perceptually informed---in order to degrade in a perceptually relevant way. Bearing a vague resemblance to our mythologies and parables around the world that have gone through steady glitch (telephone/Chinese whispers and ''I am Sitting in a Room'') and co-evolved to reflect the structure of our collective mind rather than any specific historical event.

\section{The History of MP3}
``Understanding MP3'' introduces perceptual coding with a discussion of metamers, which is a great way of describing reduced representations informed by human perception.

Lossy audio compression taking digital information that describes an audio signal and representing it with less information without producing a significantly different sound. This is called ``perceptual encoding''.\cite{Ruckert05}

Karlheinz Brandenburg's PhD dissertation formed the foundation for the MP3 format. His advisor was interested in transmitting music over telephone lines(ISDN), but the patent office said it was impossible. MP3 was 20 years in development. When asked early on ``What will happen to this?'' he replied ``It could end up in the libraries, like so many other theses, or it could be an international standard''. He adds: ``I didn't dream of hundreds of millions of people''. Fraunhofer knew they wanted to make MP3 ``the'' internet format, and took that opportunity, but didn't realize the full extent of what that meant. Now he's looking into WFS and better DRM solutions.\cite{brandenburg_interviews_2004}
	
Brandenburg admits that there are ``compromises'' in MP3, due to needing to inherit the format of MP2. ``Dry percussive material'' doesn't translate well (pre-echo effect most obvious when listening to castanets, which is not a problem with AAC). Low bitrates have ``bandpass problems at high frequencies'' which he vocally imitates as ``shw-shw-shw''. As a trained listener, he could hear the difference at 192 kbps in a blind test, but can't anymore (``I'm too old''). AAC is what MP3 ``should have been''. MPEG (motion picture experts group) was formed to put video on CD-ROMs, when at the time you had only audio on CD-ROMs (massive compression problem). Layer 2 and Layer 3 were two competing proposals during the MPEG audio format ``shootout'', and were decided to be combined. First PC-based decoders were in 1995. July 14th, 1995 they decided on the file extension ``.mp3'' For a psychoacoustic model he mentions frequency quantization and masking (In \cite{karlheinz_brandenburg_mp3_1999} he describes three ``common types of artifacts'': loss of bandwidth, pre-echoes, and roughness or double-speak.) For masking he gives the example of a train arriving at a station drowning out the conversations around you.

He says how, if you look at the data, you see runs of zeros or ones (that might be compressed by lossless compression routines like RLE), and that ``it's really a miracle with how that is reconstructed to get you the music again'' he says with a smile. He localizes different bitrates to different situations: ``128 might be good enough to listen to it on the airplane or the train''. The way the format is structured, the difference between 128 and 192 is much bigger than 256 and 320: after a certain point it takes more work to represent finer details (in \cite[9]{karlheinz_brandenburg_mp3_1999} he calls it the ``sweet spot''). This might be very loosely analogous to music creation: more people than ever are creating music right now and sharing it, perhaps exponentially, but innovation continues at a steady rate. He smiles when he says that vinyl records have artifacts that ``people come to like''. He talks about Gestalt, and how what we hear is ``clearly influenced from what we expect to hear''. Mentions how recordings can be constructed to fool people who would be able to identify their origin. People buying nice cables because they believe the sound quality will improve. He doesn't dismiss it, he says ``it's real, they think it's better, if they're happy with it then I'm fine as well''. He mentions that he knows people who have many TB of MP3s: ``which means years of uninterrupted listening, which means they have no idea what they have''. (For reference, 1 year at 256 kbps is 7.5 TB) \cite{tom_merritt_real_2010}

While PCM encoding tried to replicate the physicality of audio, as represented on vinyl and tape, MP3 reduces the information to a human-oriented format that makes an attempt at universality. In a way, it is a step towards a universal music.

Brandenburg states that we have all become ``expert listeners'' due to the simple fact that we are well trained at listening to compressed audio.\cite[9]{karlheinz_brandenburg_mp3_1999}

Brandenburg recommends limiting the response of an MP3 or AAC encoder to 16 kHz as there ``are some hints'' that there exist listeners who can identify the difference between complex signals above 16 kHz, but ``the full scientific proof has not yet been given''.\cite[10]{karlheinz_brandenburg_mp3_1999} This serves as a piece of the MP3-ideology, acts as an ontology of sound, and limits musical practice.
	
\section{The General Structure of MP3}
In order to iterate through the space of MP3, it's essential to understand how the format is structured.
	
An MP3 file consists of a bit stream that is divided into metadata\footnote{Only the audio data is defined by the ISO standard, which has lead to the proliferation of metadata formats for MP3. This is itself an interesting space, with special limitations and biases. In ID3---the most common metadata format---one byte is alloted for genre, indexing a list of 148 genres. The list includes thirteen varieties of ``Rock'', from ``Southern Rock'' to ``Symphonic Rock'' and ``Gothic Rock'', without a mention of Indian classical music, gamelan, any kind of chant, klezmer, Afro-cuban music, or baroque. Biases can be found in any ontological system, a classic example being the Dewey Decimal System's bias for Christian literature.} and audio data. The audio data is a sequence of logically independent frames physically embedded in two interleaved bitstreams: one stream occurs at regular intervals, the other fills the space in between. The regular data begins with an easily identifiable signature: a sequence of twelve binary ones, or \verb!0xfff!. This sync is part of a 4 byte header that describes the high level characteristics of the current frame, such as the number of channels\footnote{MP3 includes mono and three types of stereo encoding. Any quantitative descriptions of the MP3 format given here are in reference to the mono mode, as OELF is a mono composition.}, samplerate, bitrate, and format version.
	
Following the header is 17 bytes of ``side information'' (side info). The header and side information together make up the regular bitstream, while the ``main data'' falls in between. The main data is Huffman coded,\footnote{Huffman coding is an algorithm for compressing information. It is based on the principle of replacing more commonly occurring symbol sequences with short codes, and less frequent symbol sequences with longer codes. MP3 uses a modified version of Huffman coding that allows significant variation in the length of the symbol sequences.} and the side info provides a variety of details about how to go about decoding it. Once decoded from its Huffman representation, the main data goes through a series of complex transformations informed by various settings and scalings described in the side info.

Every frame---header, side info, and main data---contains two ``granules'' that describe two consecutive chunks of audio. Sometimes granules share some of their 21 scale factors (a sort of EQ), which is why they are stored within a single frame. Each granule stores 576 ``frequency bands'', decoded into 576 PCM samples, meaning every frame consists of 1152 PCM samples. To understand where these frequency bands come from, it may be easiest to explain them in terms of encoding than decoding. The 576 PCM samples for a single granule are first subdivided into 18 sections consisting of 32 samples each. Each block is then encoded using 32 subband filters resembling critical bands. Finally, each subband block (an 18-element time-varying sequence) is processed using a modified discrete cosine transformation that describes these time-varying sequences with respect to their regular variations. In short, the main data stored in an MP3 is a heavily abstracted representation of the PCM data. It assumes that our understanding of sound is best represented by regularly varying critical bands rather than as a time-varying sequence of variations in relative air pressure.\footnote{There is a lot of detail left out here involving quantization, pre-emphasis, short versus long blocks, global gain, windowing, overlap-add, and scale factors. All these things play a significant role in the sound of MP3, but the inverse modified discrete cosine transformation and subband synthesis are at the heart of the algorithm.}
	
\section{How Many Sounds Can We Hear?}

With the structure of MP3 in mind, under the assumption that MP3 represents all the sounds we can distinguish, we can make an approximation as to how many sounds that is. If the fundamental unit of the MP3 is the frame, and we are working at 128 kbits, every frame can have an average length of 418 bytes, or 3336 bits. In mono, 17 of these bytes are dedicated to the side information and 4 are the header. Most of the variation occurs in the main data, which is 397 remaining bytes. Therefore the total variation is $2^{397*8}=2^{3176}=10^{956}$. The number of 26 ms sounds we can distinguish is on the order of 10 followed by 956 zeros. If the fundamental unit is considered instead to be the granule, this cuts the power in half down to $10^{478}$. Considering AAC also further reduces the necessary bitrate by half, I'll place an upper bound on the number of uniquely distinguishable ``sound-atoms'' we can distinguish at $10^{239}$. The number of these sounds that can be played in a year is on the order of $10^9$, meaning that, played in succession, it would take $10^{230}$ years.
	
MP3's 26 ms is approximately in line with Bob Snyder's analysis of musical meaning and memory. He groups sonic memory into three categories: event fusion, melodic and rhythmic grouping, and form. The cutoffs for each are between 1/32 second and 1/16 second, and 8 seconds and 16 seconds respectively.\footnote{p 12, ``Music and Memory''}
	
I haven't spent enough time yet with Herbert Bruen's ``When Music Resists Meaning''.\cite{Bruen04}
	
Another perspective on musical meaning: in ``Music and Discourse'' by Jean-Jacques Nattiez\cite{nattiez_music_1990}, he describes musical meaning as a ``symbolic web''. He essentially accepts a coherentist account of meaning, saying it emerges ``when an object is placed in relationship to a horizon''. His view of this horizon is informed by psychologist Robert Franc\`es, who outlines four types of judgments:
	
\begin{enumerate}
	\item Normative judgments (such as personal taste)
	\item Objective/technical judgments (timbre, tempo and genre)
	\item Judgments about meaning (extramusical references)
		\begin{enumerate}
			\item an individual referent (connection to personal experience)
			\item concrete meaning (naturally occurring sounds)
			\item abstract meaning (playfulness, serenity or hierarchy)
		\end{enumerate}
	\item ``Affirmations of interior order'' (psychological effects upon the listener)
\end{enumerate}

Of these relationships, a number are independent of judgment: ``concrete meaning'', and arguably ``technical'' meaning as well. These relationships exist regardless of whether a conscious observer makes a judgment about them. The ``objective/technical'' relationships are suspicious only in so far as properties such as tempo and vibrato are constructs of a cultural tradition, but other properties such as relative amplitude, frequency, and timbre are more physically based rather than culturally.
	
The complexity of ``The Chord Catalogue'' compared to the MP3 specification is worth elaborating: Johnson's composition includes $2^13$ chords, while MP3 contains a serendipitous $2^13$ possible values for each of the 576 frequency lines.

[It'd be nice to have a small table with a list of the different works analyzed for their number of combinations.]
	
\section{Structurally Informed Enumeration}

	From the above section it should be apparent that MP3 is not a simple format. At least, not in the way a bitmap image or PCM file is ``simple''. To iterate through possible bitmap images, it is only necessary to pick your dimensions, bit depth, and number of channels---from there you can simply increment a binary number of equivalent length. Likewise with PCM: it is only necessary to pick your file length, bit depth, and number of channels. Unlike these formats, MP3 has a strict ``syntax'' that must be followed. Although the bitstream may appear to be a noisy sequence of ones and zeros, not just any noise is allowed.
	
	Enumerating PCM or bitmap files might be akin to the placing a monkey at a typewriter: from a fixed set of symbols, they can always pick any of the symbols. Enumerating MP3 frames is more similar to placing a human at the typewriter, and telling them they are allowed to type anything that conforms to basic rules of spelling and grammar. If conducted in Japanese, the resulting string of symbols would appear noisy to me---but to someone who understands Japanese there is a deep structure. Likewise, the hexadecimal sequence:
	
	\begin{verbatim}
	...ff fb a2 00 ea 84 43 dc 64 56 51 ec 1d 22 7b 8c 8a ca 3d 83 a4...
	\end{verbatim}
	
	Will appear noisy to the untrained eye, but to someone who understands MP3 it is obvious that this describes the header and the beginning of the side info for a stereo MP3 encoded at 160 kbps and 44.1 kHz.\footnote{Specifically, a frame from the middle of ``Cajun Waltz''; track 12 off Alan Lomax's 1934 ``Cajun and Creole Music''.}
	
	In the same way you might enumerate possible English sentences by first describing a hierarchical grammar and then providing a vocabulary, I will describe a method for enumerating MP3 frames that first accounts for their syntax, followed by the content that may be placed within that structure.
	
\section{Shuffle Enumeration}

	Summary: discuss the insufficiency of traditional enumerations at exploring a large space. Describe the shuffle enumeration technique developed for this project, and how the resulting bits were rearranged as the final compositional step.
	
\section{Difficulties of Composing for a Large Space}

	(Mention the ISO part 4 compliance testing files as examples of enumeration for technical purposes, as a comparison to this composition.)
	
	(Discuss the notion of ``composing for psychoacoustic entropy''.)
	
	(Organizing everything as tessellation: whenever you take something away from one area, you have to add it to the opposite side. Similar to bit swapping. You must necessarily compose for the entire space simultaneously.)
	
	A basic enumeration of MP3 might be compared to writing out a chromatic scale. You probably wouldn't call a chromatic scale a ``composition'', but you might call one of Shch\"onberg's 12-tone pieces a ``composition''. Once the space is enumerated, it is still waiting to be arranged in an interesting way----simply enumerating the space doesn't justify the work.

	The protagonist of Borges' ``The Aleph'' makes an interesting comment about the difference between how a work is received and how it is justified:
	
	\begin{quote}
	...Daneri's real work lay not in the poetry but in his invention of reasons why the poetry should be admired. Of course, this second phase of his effort modified the writing in his eyes, though not in the eyes of others.
	\end{quote}
	
	Perhaps this is also what John F. Simon Jr. was referring to when he wrote:
	
	\begin{quote}
	While Every Icon is resolved conceptually, it is unresolvable in practice. In some ways the theoretical possibilities outdistance the time scales of both evolution and imagination. It posits a representational system where computational promise is intricately linked to extraordinary duration and momentary sensation.\cite{john_f._simon_jr._given:32_1997}
	\end{quote}
	
	Artists like Oval and Oto have taken Yasunao Tone's technique to a different level, producing more accessible music sometimes called ``Oceanic glitch'' (as opposed to ``Conceptual glitch'' and ``Minimal glitch'').\cite{Sangild04}
	
\chapter{Conclusion}
	(This chapter will provide a brief summary of everything that was discussed.)
	
	Future work, here or in the OELF section---mention how I'd like to work with this live in a feedback situation. 24 ms latency minimum, using LAME and mpg123 to do frame-by-frame encoding/decoding. Or applying the general principle without actually using encoding/decoding, variable latency/bitrate tradeoff?

\specialhead{References}
\begin{singlespace}
\bibliographystyle{plain}
\bibliography{original,spiritofnoise,everythingart,glitchart,oelf}
\end{singlespace}

\appendix 
\addtocontents{toc}{\parindent0pt\vskip12pt Appendices}

\chapter{The MP3 Bitstream}
	
	Within the header a few sections must be considered:
	
\begin{itemize}
	\item Layer: Layer III is ``.mp3'', layers I and II are rarely used.
	\item Bitrate: In Layer III, this varies from 32 to 320 kbps.
	\item Samplerate: 32 kHz, 48 kHz, or 44.1 kHz.
	\item Mode: Specifies mono or a variety of stereo.
	\item Copyright: Whether the file is copyrighted.
	\item Original: Whether the file is original, or a copy.\footnote{For obvious reasons, the copyright and original bits are not generally used---despite the best intentions of the ISO.}	
\end{itemize}

	In a normal MP3, most of these fields will remain constant from frame to frame. OELF does not enumerate these variations, and instead chooses a single format: Layer III, 128 kbps, 44.1 kHz, mono, non-copyrighted, and original.
	
	The side info contains:
	
\begin{itemize}
	\item A description of where the main data is located in the interleaved bitstream.
	\item Scale factor sharing information: if the two granules in this frame have similar frequency-domain characteristics, they can share any of four regions of 21 scale factors.
	\item Side info for each granule
\end{itemize}

	The per-granule side info is where most of the variation exists:
	
\begin{itemize}
	\item Size of main data: describes how many bits are encoded in the main data, where the scale factors and Huffman coded data is stored.
	\item Big values: describes what portion of the main data frequency bands are ``big values''---the lower frequency components that tend to have large amplitudes. Following the big values are the ``small values'', up to the end of the main data---higher frequency components with small amplitudes.
	\item Global gain: a straightforward scaling factor applied to the entire granule.
	\item Scale factor bit allocation: an index to a table that describes how much detail the different scale factor bands are coded with.
	\item Window switching flag: Distinguishes between ``normal'' and ``special'' blocks. Normal blocks scale the entire granule with up to 21 scale factors, certain types of special blocks are divided into three short bursts with separate scale factors.
	\item A section that varies depending on the window switching flag value
	\item Pre-emphasis flag: a small concession to high frequency content, this binary flag provides a minor amplification to higher frequency components.
	\item Scale factor quantization: effectively, a multiplier flag that acts on all the scale factors.
	\item Small value table selection: specifies one of two Huffman tables to use for decoding the small values.
\end{itemize}

	There are two forms the section that varies with respect to the window switching flag can take. If the flag is false, and we have a normal block:
	
\begin{itemize}
	\item Huffman table selection: 15 bits
	\item Description of region separation: 7 bits
\end{itemize}

	If the flag is true, we have a special block:
	
\begin{itemize}
	\item Block type: 2 bits
	\item Mixed block flag: 1 bit
	\item Huffman table selection: 10 bits
	\item Subblock gain: 9 bits
\end{itemize}

	Finally, the main data consists of:
	
\begin{itemize}
	\item Scale factor bands
	\item Big valued frequency bands
	\item Small valued frequency bands
\end{itemize}

	The main data is not described in terms of bits as the bit length can vary depending on the side info specifications.

	Some features vary freely, without further constraining other features or being constrained by previous features. For example:

\begin{itemize}
	\item Global gain: 8 bits
	\item Window switching flag: 1 bit
	\item Pre-emphasis flag: 1 bit
	\item Scale factor quantization: 1 bit
	\item Small value table selection: 1 bit
\end{itemize}

	That's $8+1+1+1+1=12$ bits of variation that could be assigned randomly for any given frame without destroying the syntax.\footnote{One of my first experiments with MP3 glitching was to use these 12 bits in a composition titled ``1241385461464'', a glitched version of a recording of 4'33'' taken from YouTube. [Discuss this composition more, as there were significant realizations here.]}
	
	Then we have to decide on the window switching flags for each granule, and the block types. This in turn will constrain the scale factor sharing. This constrains the scale factor bit allocation, which has an influence on the main data size (and in turn influences the big values):
	
\begin{itemize}
	\item Size of main data: 12 bits
	\item Big values: 9 bits
\end{itemize}

	The big values must be smaller than the main data. This leads to $\sum_{i=0}^{2^{12}} min(i,2^9)$ possible combinations.\footnote{In practice, this number must be significantly smaller due to the limited space provided by the combination of the bit reservoir and the bitstream following the side info.}
	
	One piece remains: the main data location. Because the main data location is solely physical (bitstream-oriented) rather than logical (sound-descriptive), it can be derived rather than enumerated.

\end{document}
