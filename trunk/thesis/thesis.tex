\documentclass{thesis}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}

\thesistitle{\bf Only Everything Lasts Forever}        
\author{Kyle McDonald}        
\degree{Master of Fine Arts}
\department{Electronic Arts}
\projadviser{Curtis Bahn}
\coprojadviser{Michael Century}
\cocoprojadviser{Shawn Lawson}
\submitdate{April 2010\\(For Graduation May 2010)}
\copyrightyear{2010}
      
\begin{document} 
\titlepage
\tableofcontents
%\listoftables % required if there are tables
%\listoffigures % required if there are figures

\specialhead{Acknowledgments}

(Insert acknowledgments here.)

\specialhead{Abstract}

Only Everything Lasts Forever is a sound composition containing every sound we can distinguish. It explores the social and political associations of sound representation along with the psychology and philosophy of noise and emptiness.

\chapter{Introduction}

Noise is an invention. Not only an invention, but a necessity. Noise is misunderstanding and non-contextuality. As long as there are observers, there will be noise---and without observers, there is no noise.

Historically, noise has had a number of definitions. A close definition to mine is ``that outside of a given representation'', given by Doug Van Nort in ``Noise/music and representation systems''\cite{Vannort06}. Or consider the definitions of Claude Shannon, Luigi Russolo\cite{Russolo04}, John Cage\cite{Cage61}, Jacques Attali\cite{Attali85}, Kim Cascone\cite{Cascone00}, Douglas Kahn\cite{Kahn01} and Paul Hegarty\cite{Hegarty02}.

I propose an alternative definition of noise built on the notion of \emph{non-contextuality}. Rather than placing noise in the usual opposition to music, I will explore this directly. First, I will present a number of examples that outline the sense of ``context'' I'm describing. Then I will discuss art works focused on collaborating with and revealing implicit contexts. Finally, I will describe my own work, ``Only Everything Lasts Forever'', and its exploration of the context of the MP3 format.

\chapter{Understanding Context}

\section{Metaphor, Coherence and Interdependence}

In ``Metaphors We Live By'', George Lakoff and Mark Jonhnson explore the thesis that our beliefs and actions are structured by a metaphorical ``web'' of understanding. The word ``context'' is itself rooted in a metaphor: the Latin ``contexere'' describes the ``joining together'' of weaving. In this paper, the word ``context'' refers to this idea of a system that is supported by intertwined relationships rather than a single foundation. These relationships may be self-referential, as Douglas Hofstadter argues in the case of cognition\cite{Hofstadter01}\footnote{Hofstadter even continues, describing not only cognition as analogical, but consciousness itself as fundamentally self-referential\cite{Hofstadter07}.}. In accordance with metaphor as the foundation for understanding: when we have no context for a sound, we cannot conceptualize it so we call it noise.

In Western philosophy, ``context'' manifests itself as the coherence theory of truth\cite{Blackburn07}\cite{young_coherence_????}, where truth is identified by its coherence with other propositions. This theory stands in opposition to the correspondence\cite{david_correspondence_????} theory of truth, where truth is identified by correspondence to an external system. In accordance with coherence as the foundation for belief: when we have no context for a sound, we cannot identify its meaning, so we call it noise.

In Eastern philosophy, ``context'' manifests itself in the Buddhist doctrine of \emph{pratityasamutpada}, or ``interdependent arising''\footnote{Cage uses the translation ``interpenetration'' to describe this concept.}, which accounts for the causal relationship between all things.\cite{Koller01} This is deeply tied to the notion of \emph{sunyata}, or ``emptiness'', which asserts that nothing has an inherent essence but exists only in relation to everything else. In accordance with interdependence and emptiness as metaphysical truths: when we have no context for a sound, there is no essence to be found, so we call it noise.

\section{A Signal Appears as Noise}

\begin{quote}
Take a photograph of your head inside a freezer. Upload this photo to the internet (like flickr). Tag the file with 241543903. The idea is that if you search for this cryptic tag, all the photos of heads in freezers will appear. I just did one.
\end{quote}

Sometimes the deeper context of apparent noise is later revealed. The above instructions were sent by David Horvitz to a mailing list in 2009\footnote{\url{http://davidhorvitz.com/2009/}}. To a casual surfer, this unusual photo paired with a ``cryptic'' number\cite{david_horvitz_flickr:_????-1} would appear as noise. But once the context of Horvitz' instructions are revealed, the image and tag are no longer understood as noise. Alternatively, once enough photos are surveyed with the same theme and tag, the context becomes the set of images.\footnote{Also see ``MOOKDFJLAL''\cite{david_horvitz_flickr:_????}, where Horvitz encourages people to put pillows on their heads.}

In the case of a tagged image, you might suspect a deper context than the superficially obvious one. In computer security, apparently indecipherable noise can sometimes be reverse engineered to reveal a deeper context where none was originally even suspected---a context beyond intuition, but computationally accessible. These ``side-channel attacks'' might consist of simply zooming in on a reflection of a screen at a distance to snoop on the displayed information,\cite{w._wayt_gibbs_hackers_2009} or recording the emitted RF radiation of a CRT monitor and amplifying it for display on another screen.\cite{erik_thiele_tempest_????} Variations in a computer's sluggishness can reveal the contents of its computation (called a timing attack). The LED on a router may appear to blink at random, but they are sometimes driven directly by the data line---and can therefore be optically recorded and decoded.\footnote{Similar computer security techniques were infamously applied to daily life, as ``reality-cracking''\cite{francesco_vianello_reality_????}, by Fravia (Francesco Vianello)---a prominent software cracker who regularly published various technical details of his practice along with manifestos on cracking philosophy. He proposed: the same way line after line of assembly code may at first appear impenetrable and unrelated to the program it represents, the chaos of daily life may seem irrelevant to our belief systems and practices and those of others around us. But they are related, and it only takes time to understand the ``concealed truths''. Fravia was particularly focused on ``seeing through'' consumerism, but was interested in everything---including body language, food additives, war propaganda, telemarketing, and bottled water.}

\section{Noise Appears as a Signal}

Summary: Other times, the boundary between noise and signal is unclear because what we filter noise until only a signal is left.

We seem to have a penchant for anthropomorphizing astronomical structures, from the face on mars\cite{brian_dunning_facemars_2008} to the CfA2 Great Wall in ``A Slice of the Universe''\cite{de_lapparent_slice_1986} resembling a humanoid skeleton that became known as the ``stick man''. Both of these structures failed to hold up to more in depth scrutiny.

The photo collection ``Iron Jesus, and other religious signs''\cite{boston.com_religious_????} includes: a cross on the head of a calf, a face in the wood floor of a chapel, Jesus on a fish stick, Saraswati in the clouds, and Mary on a door stain. Dan Paluska's ``Holy Toaster''\cite{dan_paluska_holy_2005} is an art object and commercially available toaster insert that burns the face of Jesus onto every piece of toast inserted. Melinda Green's ``Buddhabrot''\cite{melinda_green_buddhabrot_1993} technique for rendering the Mandelbrot set was originally named ``Ganesh'' after it was recognized by an Indian coworker.

Seeing faces and hearing voices. ``Ghosts in the Machine'' by Alan Dunning, Paul Woodrow, and Dr. Morley Hollenberg\cite{alan_dunning_paul_woodrow_and_morley_hollenberg_einsteins_2008} explores electronic voice phenomena---situations where controlled white noise are interpreted as human voices. In the installation, a camera is placed inside a black box, the image formed on the sensor is amplified in software and automatic face detection algorithms are applied. The computer then projects the image from the camera with a rectangle around any region identified as containing a ``face''.

Sometimes we project extrinsic structure, as in the Kanizsa\cite{alexander_bogomolny_kanizsa_????} triangle illusion.

Context doesn't have to be ``built in'', but can be acquired. The dalmatian illusion\cite{michael_bach_dalmatian_2002} is a great example of finding order in noise: a seemingly random field of black and white regions is presented, but the dalmatian eventually kicks in and you can't see it any other way.

One of the more infamous recent examples of pattern-finding comes from ``Equidistant Letter Sequences in the Book of Genesis''\cite{rips_equidistant_1994}, popularly presented as ``The Bible Code''.

Applied computationally: Jonathan Feinberg's ``Haiku Finder''\cite{jonathan_feinberg_haiku_????} automatically detects sentences that fit the English 5, 7, and 5 syllable rules.

The Voynich manuscript is a 240-page document more than 400 years old, filled with exotic illustrations and an untranslated text. Currently, one of the leading theories of the work is that it is a hoax based on a simple algorithm for constructing a false language that resembles a real language.\cite{robin_mckie_secret_2004}
		
Doczi's ``Power of Limits''\cite{Doczi81} is dedicated to recovering the many natural manifestations of the golden ratio, $\phi$, and analyzing various man-made constructions in similar terms. Markowsky's ``Misconceptions About the Golden Ratio''\cite{markowsky_misconceptions_1992} deals with the fallacies of this approach,\footnote{Especially: the Pyramidology fallacy. Moore\cite{Moore07} is a good reference for these common fallacies.} and cites numerous cases that have been inaccurately represented as $\phi$-resembling (the Great Pyramid, the Parthenon, da Vinci, the human body). While historical usage is dubious, modern use is explicit and exemplified by Le Corbusier's Modulor.\cite{padovan_proportion_1999}
		
Prime number finding has a very long tradition of pattern-finding, especially spiral-based techniques such as Sacks\cite{michael_m._ross_natural_2007} and Ulam\cite{weisstein_prime_????} spirals, which ``[appear] to exhibit a strongly nonrandom appearance''.
	
SETI@home\cite{seti_about_????} is a distributed computing project developed at UC Berkeley focused on discovering signs of extra terrestrial intelligence in radio transmissions. Key to this investigation is the distinction between a ``sign'' and ``noise''---without a goal, there could be no search. This distinction was agreed upon by researchers, and is realized in the physical configuration of the Arecibo telescope and the software that processes its signal.

While most ``natural astronomical sources'' produce broadband noise, SETI suggests that our ``stellar friends'' would use a narrow bandwidth signal to differentiate themselves.\footnote{SETI@home has an additional filter for a physical phenomena that regularly influences their measurements: Doppler shift. Due to the movement of planets, any extra terrestrial signal would be changing in frequency at a steady rate over the 12 second measurement. This is accounted for by a ``de-chirping'' step.} Furthermore, in order to encode information in the signal they would likely use regular pulsing---amplitude modulation. They provide two solutions for finding these pulses: look for strong triplets, or look for weak but regular pulses.
	
SETI has identified the most basic meaningful signal as self-referential and repetitive. Events often occur for unidentified reasons (single pulses), and therefore there is a high probability that multiple similar events will occur at different times. However, if they're unrelated, it's unlikely that there will be any correspondence in the timings. The shortest sample of events that yields timing-correspondence is three events---and the idea of regular pulses is simply an extrapolation of that principle to longer portions of time. It becomes clear that SETI has implicitly defined intelligence as the ability to order events in regular intervals.\footnote{It follows that the most common false positives are found in pulsars: very quickly rotating neutron stars that emit regular bursts of radiation---decidedly non-intelligent structures. The currently unidentified 1420 MHz radio source SHGb02+14a between Pisces and Aries is assumed to be a pulsar.\cite{eugenie_samuel_reich_mysterious_2004}} This is an example of self-referential meaning.
	
\chapter{Revealing Implicit Contexts}
	
	Summary: A context is always present. Sometimes the message is communicated on a biased media (the signal has distortion?), other times the receiver is biased.

\section{Empty Art}

	Summary: Empty art has made one attempt at revealing the context inherent to a medium.

	Borges' short story ``The Aleph''\cite{borges_aleph_2004} deals very directly with the theme of ``everything'', a corollary to emptiness. The titular subject of ``The Aleph'' is a visual phenomena: a point in space from which you may see all other points in space, ``the only place on earth where all places are --- seen from every angle, each standing clear, without any confusion or blending''. There is a very interesting note near the end about a sonic corollary:
	
	\begin{quote}
	The Faithful who gather at the mosque of Amr, in Cairo, are acquainted with the fact that the entire universe lies inside one of the stone pillars that ring its central court... No one, of course, can actually see it, but those who lay an ear against the surface tell that after some short while they perceive its busy hum... The mosque dates from the seventh century; the pillars come from other temples of pre-Islamic religions, since, as ibn-Khaldun has written: ``In nations founded by nomads, the aid of foreigners is essential in all concerning masonry''.
	\end{quote}
	
	Carlos Argentino Daneri is a writer who discovers the Aleph in his basement, and is using it to produce an epic poem titled ``The Earth'', describing everything in detail. While explaining the structure and content of the poem, he links it to scripture with all of its ``enumeration, congeries, and conglomeration''. In a postscript to the story, the fictionalized Borges mentions the origin of the Aleph's name:
	
	\begin{quote}
	As is well known, the Aleph is the first letter of the Hebrew alphabet. Its use for the strange sphere in my story may not be accidental. For the Kabbalah, the letter stands for the En Soph, the pure and boundless godhead; it is also said that it takes the shape of a man pointing to both heaven and earth, in order to show that the lower world is the map and mirror of the higher...
	\end{quote}
	
	In Kabbalah, there is also an intricate internally consistent numerological interpretation to the character $\aleph$.\footnote{\url{http://gnosticteachings.org/courses/alphabet-of-kabbalah/01-aleph}. A wonderful exposition of the psychological state required for intense numerological research can be found in the film $\pi$.} The aleph is seen as representing the Kabbalistic Holy Trinity, and this ``explains'' the facts that: $\aleph$ represents the number 1, or unity, and appears at the beginning of the Hebrew aleph-bet; the spelling of its name consists of three characters---the Trinity; finally, the values of these characters---80, 30, 1---sum to 111, or three alephs. The aleph is also associated with the breath of life---an interesting connection given the ``Aum'' of Hinduism that may also represent breath, unity, and the creative force of the universe.
	
	Cage's 4'33'' is the perfect example of ``empty art'' in the context of sonic art.\cite{larry_j_solomon_sounds_1998} Both 4'33'' and OELF have an emphasis on allowing the listener to trust themself. But 4'33'' is primarily concerned with the human experience while OELF is situated in the context of an extra-human ``nature''.
	
	Why were some people so mad at Cage after the first performance of 4'33''? Being presented in the context of a musical performance, he was associating his musical work of silence with the musical work of others. Empty work---4'33'', Duchamp's ready-mades, Rauschenburg's white paintings---destroy the artist by elevating the individual. Historically the Western artist has acted as a sort of aesthetic-ubermensch, creating a framework of perception for their audience: a framework rooted in religious interpretation and self-expression, before and after the Renaissance respectively. Without bad faith comes existential angst.\footnote{Sartre, Being and Nothingness} Cage seemed to be somewhat unaware of this effect, saying:
	
\begin{quote}
	Many people in our society now go around the streets and in the buses and so forth playing radios with earphones on and they don't hear the world around them. They hear only what they have chosen to hear. I can't understand why they cut themselves off from that rich experience which is free.
\end{quote}

	Not everyone is comfortable determining their existence and asserting their freedom.
	
	In 1957, with the avant-garde in full force exploring chance operations, graphic scores, improvisation, and atonal composition, Cage writes, ``Try as we may to make a silence, we cannot, one need not fear for the future of music''. The irony of this statement is that no one was worried about just making more sounds. Cage is responding to the fear of these developments with an encouragement to open our ears: as long as we can't make silence, there is music to be made.
	
	Cage defines silence as ``giving up of intention''. As we are the only beings capable of giving up intention, it follows that we create not music, but silence. Cage goes on, saying ``Music is continuous. It is only we who turn away''.
	
\begin{enumerate}
	\item Cage sees music and silence as exclusive, distinguished only by our intention/attention.
	\item I'm considering noise and non-noise as exclusive, with noise being non-contextual sounds and non-noise being contextual sounds. Music represents a subset of contextual sounds. Noise doesn't exist without us, but neither does music. The problem with Cage's separation is sounds that you are fully attentive to but refuse to understand as musical.
\end{enumerate}

That is, noise is non-contextual sound, without meaning, while contextual sound is meaningful but not necessarily music. These contexts are biologically and socially informed.

``White on White'' by Kazimir Malevich is totally non-representational: a skewed white square against a white background. It was painted in 1918, five years after his form-oriented ``Black Square''. Leah Dickerman at MoMA says of ``White on White''\cite{moma_kazimir_2006}:
	
	\begin{quote}
	White is a way of taking away---minimizing---color itself, and actually focusing particularly on the material of painting.
	\end{quote}
	
``White on White'' is not about painting in general, but specifically about Malevich's painting: you see his brush strokes. He described his art as ``Suprematist'', focusing on ``the supremacy of pure feeling or perception in the pictorial arts''. From an evolutionary perspective, Suprematism would imply representationalism in that our perceptions are attuned to nature. Malevich takes a more spiritual and idealistic route, dismissing the feelings associated with ``real objects''. He explains in ``From Cubism and Futurism to Suprematism'' that representationalism (``The transferring of real objects onto canvas'') is copying, and a genuine act of creation only occurs when an artist's ``pictures have nothing in common with nature''. He maintains that the medium itself is the most important part of painting:
	
	\begin{quote}
	For art is the ability to construct, not on the interrelation of form and color, and not on an aesthetic basis of beauty in composition, but on the basis of weight speed and the direction of movement. [...] Color and texture in painting are ends in themselves. They are the essence of painting, but this essence has always been destroyed by the subject.
	\end{quote}
	
It's interesting that these first Suprematist works of Malevich (``Black Square'', ``Red Square'', ``White on White'') used squares, which he called a ``zero form''. More geometrically elegant is the circle. More intuitive to paint is the line, or even the point. But he focused on the square (and later, rectangles), in a strange foreshadowing of raster images. Because these pieces were about filling the space of the canvas, it must have been an intuitive response to the squareness of the canvas---itself informed by the physical biases of construction. In the same way, screens have been arranged in rectangular matrices (rather than hexagonal or any other pattern) for manufacturing and electronic reasons.
	
	\cite{moma_rodchenko_1998}
	In 1921 Alexander Rodchenko first exhibited his three canvases, ``Pure Red Color, Pure Yellow Color, and Pure Blue Color''. These three canvases were the first true monochromes. He later said of them:
	
	\begin{quote}
	I reduced painting to its logical conclusion and exhibited three canvases: red, blue and yellow. I affirmed: it's all over. Basic colors. Every plane is a plane and there is to be no representation.
	\end{quote}
	
Rodchenko deals with painting here directly as a medium: the core of painting with respect to color is the principle of primaries in pigment-based subtractive color theory.
	
Alphonse Allais' white painting ``Anaemic Young Girls Going to Their First Communion through a Blizzard'', black painting ``Negroes Fighting in a Cave at Night'' and 24 empty measures in ``Funeral March''. People weren't ready yet, these pieces are rarely mentioned though they are just as conceptually rich as Rauschenberg's white paintings, the Russian painters that preceded him, and Cage's 4'33''.
	
Why the strong association with death---``Funeral March'', ``The Death of Painting'', ``The End''?\footnote{The topic of ``the end of art'' needs discussion of Arthur Danto + his treatment of Hegel.} Did the artists themselves have the same response to their work as Cage's audience to 4'33''? That without an absolutist correspondence-based account for art, there could be no more art? Yes, there seems to be significant difference between these approaches to emptiness. ``Funeral March'' wasn't just silent in Cage's sense, but it was composed for a deaf man. 4'33'' cannot be audited by a deaf person, as it is about embracing sound. Cage even described 4'33'' as ``the beginning of music'', clearly seeing the matter in the opposite light. John F. Simon Jr. says that ``Every Icon'' was a beginning as well:
	
	\begin{quote}
	There was a lot of talk at the end of the 80's when post-modernism was emerging about how we've reached the end of imaging, and I wanted to show that even in a simple 32-by-32 space, the possibilities for imaging were vast.\cite{matthew_mirapaul_in_1997}
	\end{quote}
	
	Rauschenberg's white paintings gave Cage ``permission'' to write 4'33''. The white paintings offer a significant metaphor for understanding 4'33'': not as ``empty'', but as ``mirrors''. Perhaps ``Mirror Art'' is more fitting than ``Empty Art'' when describing this collection of pieces.
	
	Cage tied this realization of music's nature, intentional sound, to music's function: a process of discovery, becoming more attentive to sounds. When describing noise and non-noise, I understand neither in a functional sense.
		
	Composer Peter Ablinger has a number of writings dealing with white noise as the sound that contains all sound. He focuses on a sort if intra-coherentism, or finding meaning in self-similar works. He writes, ``redundancy produces information''\cite{christian_scheib_statics_????} and has some other helpful reflections on the relationship between noise, repetition, and meaning. Christian Scheib writes about Ablinger:
	
	\begin{quote}
	It was Arnold Sch\"onberg who stated that variation is necessarily a form of repetition as at least something must return between variation and variation. And he thus shifted his attention from the changing aspect of music to the continuous, the repeated. Ablinger's attitude is something like the hidden, reverse secret of the same story: that each repetition is also variation, that there is necessarily always something changing---that, in other words: repetition does not exist except as an abstraction. 
	\end{quote}
	
	[Sch\"onberg's sense of ``development'' is obviously far removed from Ablinger's more philosophical sense, so maybe only the second half of the above quote belongs---and the Sch\"onberg reference can be dropped.]	And Ablinger speaks for himself in the notes for IEAOV:
	
	\begin{quote}
	Human beings are able to think of simultaneous things as successive ones. This is thinking. Thinking is making a successive order out of the surrounding whole, out of totality. Thinking therefore might be thought of as the negation of simultaneity. Thinking then is the negation of any actually accessible lived experience.
	\end{quote}
	
	The relevance of this idea of simultaneity versus succession becomes clear when Ablinger discusses his ``Rauschen'' series:
	
	\begin{quote}
	Rauschen (White noise) is the totality of sounds---``everything always'' in its acoustic representation. Comparable to white light that contains all colours, white noise contains all frequencies, and - poetically speaking - all music. [...] The reason why we hear ``less than nothing'' is that we cannot connect to it by just listening. It is simply too much. We can't do anything with it. The only thing that is left to do is to produce illusions, i.e., to hear something ``in'' the noise that is not there, that can be perceived only individually - to project our own imagination onto that white ``screen''. In this way Rauschen works like a mirror, reflecting back only what we project onto it. \footnote{\url{http://ablinger.mur.at/rauschen.html}}
	\end{quote}

	[This idea of meaning emerging from self-reference in music drove my own explorations of automated looping systems in 2007.]
	
	A great example of exploiting this phenomena in the visual field is the Monochrome works by RYbN\footnote{\url{http://www.cimatics.com/festival2008/festival/performances/aka_pfmonochrome.html}} where ``black'' video is played in complete darkness, revealing biases of the encoding/decoding process; or the 20 kbps music label\footnote{\url{http://20kbps.sofapause.ch/}}, focused on releasing the lowest fidelity MP3 encoded music.
	
\section{Glitch Art}
	Summary: an alternative approach to revealing a medium can not be found by representing the ingredients, but by making minor changes to an encoding in the medium's space that has major effects in the perceptual space. This is glitch.
	
	Lossless compression refers to a method of rewriting information. It takes a long signal of data with low information content and rewrites it as a short signal with high information content. In information theory, when a signal looks very noisy and random there is high information content, and when there is obvious structure there is low information content. At first, it might seem like the idea of ``information'' in information theory is almost completely opposite our normal intuition for ``information''.
	
	To relate our intuition to the formal definition, let's consider the question: how much is communicated by each symbol in a stream? Here is a binary description of a coin toss event with one unknown symbol indicated by an asterisk: 100*011. Now consider the same event described in English with many unknown symbols: h**ds t**** t**ls **a*s t*i*s he**s **a**. The fact that we can remove so many symbols from the English description while maintaining the integrity of the message means that each symbol individually communicates a small amount of information. On the other hand, removing a single symbol from the compressed binary representation destroys an entire toss.
	
	Lossy compression also has the property of reducing the size of the signal, but it makes a concession: a signal that is lossily compressed can not be used to perfectly recover the original signal. Lossy compression relies on the fact that some data is more relevant to the signal's general message than other data. For example, given a recording of human speech, we can discard anything above 8 kHz and still get the general message. Depending on the representation, that low pass filter could allow us to halve the size of the compressed signal.
	
	While lossless compression is essentially applied math---dependent primarily on the assumption that various media can be represented digitally and therefore analyzed for redundant information like any other binary data---lossy compression represents a significantly different interpretation of the digital signal. Lossless compression algorithms are purely syntactic, but lossy compression considers the signal's semantics in the context of the receiver. Lossless compression assumes only that a medium can be digitized, but lossy compression goes one step further and defines an associated space for the medium. In theory this is space is directly related to human perception, but in practice it is standardized by small groups of specialists.
	
	This approach to lossily transcribing sound is much more akin to the shorthand used by ethnographers and composers than it is to vinyl, magnetic tape, or PCM format audio in that it treats the audio as a perceptual stimuli rather than a physical event.
	
	The databenders group on Yahoo\cite{indianropeburn_databenders_????} was started on May 5th, 2001, now described as ``Sound Synthesis using Raw Data. Also including discussions of cd-bending, data-to-image, image-to-sound and other related techniques''. It was founded by Robert Green, who now runs Alien-Devices, which ``has been making professional quality modified and circuit bent instruments for over a decade'' http://alien-devices.com/ The group saw a lot of interest in early 2002, steadily declining through 2006. More than a thousand messages document various experiments in renaming files and using ``raw'' formats for converting between sounds and images. It's mostly practical discussion about which software to use, settings that sound or look interesting, and techniques for post-processing found data. Writing custom software happened, but was less common than using off the shelf software in unconventional and unintended ways.
	
	The databenders stumbled across a number of basic practical observations regarding how different formats represent visual and sonic information. This understanding of lower level structures guided their explorations, sometimes crossing over into glitching rather than transcoding, but always revealing something about the underlying medium itself.
	
	Cage was a sort of databender by default. Referring to his interest in graphic scores and process-driven composition:

\begin{quote}
...since I can't hear [music] while I'm writing it, I'm able to write something that I've never heard before...
\end{quote}

	Cory Arcangel's gradient works\cite{cory_arcangel_photoshop_2009} explore the influence our tools have on us. The use of the gradient and smudge tools have significantly affected design work of the last few decades. Using only those tools, he constructs an image that is too familiar and repulsively simple.
	
	Yasunao Tone's ``Wounded CD''\cite{media_art_net_media_2010} work represents a glitch art that explores its own medium. ``Wounded Man'yo'' specifically represents the culmination of a number of transcoding and glitching practices. Tone starts with the ancient Chinese Man'yo texts, and selects pictures that correspond to each Chinese symbol. These pictures are then irreversibly transcoded: each row and column has the histogram calculated, and these are arranged in a linear order. This is done for every image, and resulting signal is burned onto a CD. The CD is then wounded with scratches and other modifications. The resulting sound might be considered an exposition of two representational modalities: the histogram-based image representation, followed by the CD-based sound representation. Because histogram-based image encoding is non-standard, we hear the result primarily as noise, with the exceptions of the CD glitches that reveal the structure CD/CD player system.
	
	Artists like Oval and Oto have taken this technique to a different level, producing more accessible music sometimes called ``Oceanic glitch'' (as opposed to ``Conceptual glitch'' and ``Minimal glitch'').\cite{Sangild04}
	
	Glitch art as a method for exploring a medium is something like musical improvisation in an unusual space. Before you begin, there is the ``silence'' of the space: the sound of 4'33'' or light of a white canvas. With your first sonic gesture you create a wave that is interpreted and reflected back to you by the room, corrupting the silence and revealing something about the rooms structure and biases. In the same way, glitch art is sometimes about making a gesture within a well defined medium: corrupting the natural content of a file in order to reveal its structure.
	
	The Glitch Art Pool\cite{liminalmike_flickr:glitch_????} on the image-sharing website Flickr has been around since (insert date). It represents a significant body of collective recognition and awareness of glitches. New contributions are regularly submitted by individuals who, in the moment of a system's failure, recognize the glitch as beautiful and desire to share it. The Glitch Art Pool is relevant in that it represents the practical awareness of noise and the structure of encoding/decoding systems.
	
	``The Ceibas Cycle'' by Evan Meaney\cite{evan_meaney_ceibas:_2008} approaches digital video as a container for a memory that will steadily fade over time. The work is relevant in that he addresses 27 different codec/container combinations and glitches them all in a similar way: substituting the ASCII representation of their data with some plain text. The resulting videos act as a record of the format's biases and assumptions about perception: some display optical flow artifacts, others quickly strobe magenta when color information goes missing.

	In ``Phantoms in the Brain''\cite{ramachandran_phantoms_1999}, V.S. Ramachandran espouses the notion of looking at where things break to understand how they work. When many patients have lesions in a similar area of the brain, and they all exhibit similar disabilities, it is assumed that that region of the brain is responsible for the missing behavior. More interesting than behavior localization is exploring the mechanisms for normal operation: the idea of a malleable body-image and it's ``phantom limb'' glitch, the notion of ``filling-in'' happening regularly in our blindspots at the optic nerve (or sometimes more extremely in the case of cataracts and blindsight), etc. The brain can experience glitches that reveal its inner workings the same way any other media might.
	
	``Download Finished'' by Mediengruppe Bitnik and Sven K\"onig\cite{!mediengruppe_bitnik_and_sven_knig_download_????} automatically downloads movies from peer to peer filesharing networks, ``transmogrifies'' them, and redistributes the modified versions. They describe it as ``found art'' and claim that their ``transmogrified'' films ``belong to everyone''. Their transformation is very important to the result, as they feel it will ``reveal the nature of the found footage files as a collaborative work with a very complex data structure''. Their mode of glitching (removing key frames) is less perceptually oriented than systems oriented:
	
	\begin{quote}
	A film found in a filesharing network is the sum of $1$ the original film, $2$ the work of the mathematicians who laid the theoretical foundations for $3$ the programmers who designed the encoding software / the codec and $4$ the file sharer who finally uses all that software to intentionally make the $5$ film widely available. The processes behind $2-4$ usually stay invisible, leading to the wrong assumption that $1=5$. DOWNLOAD FINISHED transformes[sic] $5$ such, that the processes behind steps $2-4$ become visibile[sic] and show that films found in file sharing networks are actually collaborative works.
	\end{quote}
	
	John Michael Boling provided an open forum for discussion of the ``pixel bleed'' effect on the Rhizome blog\cite{john_michael_boling_rhizome_????}, and saw a number of responses noting various artists who have worked with the technique. It starts with the advent of lossy compression itself, developing into the most recent mainstream Kanye West and Chairlift music videos. This discussion moves away from the politics and biases of the compression, and towards the ownership and connotations of the technique.
	
	``No Noise'' by Jochem van der Spek\cite{jochem_van_der_spek_no_2001} is heavily related to MPEG video glitching in that it explores our ability to objectify motion of static noise but not noise itself. My own variation on this theme\footnote{\url{http://openprocessing.org/visuals/?visualID=7072}} replicates the situation, but for an object of a constantly varying size.
	
	The Moscow State University Graphics and Media Lab maintains this interesting page of glitches\cite{nikolai_trunichkin_and_dr._dmitriy_vatolin_crazy_????} they've stumbled across while developing their filter library (a collection of small visual processing modules used for research purposes). The unusual thing here is that they've made remarks on exactly which part of the algorithm has gone astray during the implementation. This is the other end of the Databender's Wordpad-mod scene: instead of making uninformed and intuitive modifications to a file in a non-representative format, the MSU Graphics and Media Lab has very precisely implemented the necessary algorithms and documented a few cases where something identifiable has gone wrong.
	
	MPF (``MPeg Fucker'') by imbecil\cite{imbecil_mpeg_2004} follows the general tradition of glitch-alike by making ASCII substitutions in the file. For example, substituting `a' (0110 0001) with `9' (0011 1001). Two scripts are provided for glitching files in slightly different ways, distinguished only by the choices of letter combinations for substitution. There is a bit of structural information represented here in that the substitutions themselves are fixed. The substitutions aren't made at random, but some favorites are encoded and shared.
	
	(Also discuss Mats \"Oljare's work with MP3 glitching by search/replace and in audio editors here.)
	
\section{Enumeration and Permutation}
	
	Summary: Another way to express a space is by enumerating all of its combinations.

	Borges' short story ``The Library of Babel''\cite{borges_library_2000} deals with the theme of enumeration.	In ``The Library of Babel'', the main character tells of the unusual universe in which he lives: floor after floor of interconnected hexagonal rooms, lined with rows of books on shelves. These shelves are said to contain all the possible 410-page books (40 lines per page, 80 characters per line, from an alphabet of 25 characters). He outlines what this implies regarding the contents of the books:
	
	\begin{quote}
	Everything: the minutely detailed history of the future, the archangels' autobiographies, the faithful catalogs of the Library, thousands and thousands of false catalogs, the demonstration of the fallacy of those catalogs, the demonstration of the fallacy of the true catalog, the Gnostic gospel of Basilides, the commentary on that gospel, the commentary on the commentary on that gospel, the true story of your death, the translation of every book in all languages, the interpolations of every book in all books.\footnote{In a 2001 post to alt.math.recreational, Keith F. Lynch elaborates further on what exactly is contained in ``everything''.\cite{keith_f._lynch_converting_????} He author offers a long list as a warning to the original poster who asks about representing $\pi$ in binary, warning them they will be ``guilty of: Copyright infringement, Trademark infringement, Possession of child pornography, Espionage (unauthorized possession of top secret information)[...] Also, your computer will contain all of the nastiest known computer viruses''.}
	\end{quote}
	
	Responding to critics who claim that the library can only ``affirm, negate and confuse everything like a delirious divinity'', the author writes:
	
	\begin{quote}
	In truth, the Library includes all verbal structures, all variations permitted by the twenty-five orthographical symbols, but not a single example of absolute nonsense. [...] I cannot combine some characters, ``dhcmrlchtdj'', which the divine Library has not foreseen and which in one of its secret tongues do not contain a terrible meaning. [...] The Library is unlimited and cyclical. If an eternal traveler were to cross it in any direction, after centuries he would see that the same volumes were repeated in the same disorder (which, thus repeated, would be an order: the Order).
	\end{quote}
	
	No book can be nonsense precisely because it exists in the context of an alphabet, in the context of a book, and within the Order of the Library itself. This context is the meaningfulness. Furthermore, in this self-contextualization through enumeration emerges the realization that every possible language is represented, and that this offers another degree of context.
	
\subsection{Pre-enumeration}

	Wright\cite{Wright09} describes systems artists as the ``the last programmers before the digital computer made that practice synonymous with its own functioning''. At the boundary of systems art and modern-day programming were pieces like Paul Brown's ``Lifemods'' in the late 70s:
	
	\begin{quote}
	Visually the work was formally more sophisticated and texturally richer than previous Systems work, but most striking was the sheer quantity of graphics and audio that was now being produced, something approaching a continuous torrent of sensory data.
	\end{quote}
	
	Since 1987, Alan McCollum has been exploring large numbers and the limits of human-created unique objects with his ``Shapes Project''.\cite{allan_mccollum_shapes_2006} McCollum aims at creating one ``shape'' (a solid black object against a white background) for every person on the planet. Rather than design a program to generate these shapes, McCollum is creating them by hand. To make sure he never repeats the same shape twice, he's constructed a complex shape-generating system that he follows rigorously.
	
	[Perhaps discuss random sampling as pseudo-enumeration?]
	
\subsection{One Dimensional Spaces}

	\cite{michael_aschauer_8-bit_????}
	``8-BIT'' by Michael Aschauer consists of 8 fluorescent tubes steadily counting off all the possible 8-bit numbers using a binary enumeration. A significant part of the piece resides in the unpredictable nature of fluorescent lights, leading to certain combinations lasting only a brief moment.
	
	Sebastian Tomczak has done some work with enumeration in a sonic domain, working with loops of short PCM encoded waveforms. ``All 4-bit Waveforms That Have 32 Samples''\cite{tomczak_all_2009} expands into a space of $2^{4^{32}}=16^{32}$ possible samples at 440 Hz, which ``will take 1,079,028,307,100,000,000,000,000,000 centuries to complete''. His ``Hardware-based Waveform Permutations''\cite{tomczak_hardware-based_2009} implements the same concept in hardware with counters and multiplexers, but for only 4 samples instead of 32.
	
	Working with twelve notes of an octave rather than 32 samples, Tom Johnson's ``The Chord Catalogue''\cite{tom_johnson_liner_1999} performs a similar iteration through possible sounds: the 8178 possible chords in an octave. The composition uses a similar enumeration as ``De Wensput'': first all two notes intervals, then three note chords, then four notes, etc. An interesting historical precursor is mentioned in the liner notes:
	
	\begin{quote}
	Already in the early 17th century, the intellectual monk Marin Mersenne posed questions in the Livre du chant of his Harmonie universelle as to the number of possible melodies one could construct by changing the order of notes in a 22-note sequence. After much calculation he concluded that there was no point in actually carrying out this experiment, since one could never hear that many melodies in one lifetime.
	\end{quote}
	
	The complexity of ``The Chord Catalogue'' compared to the MP3 specification is worth elaborating: Johnson's composition includes $2^13$ chords, while MP3 contains a serendipitous $2^13$ possible values for each of the 576 frequency lines.

\subsection{Two Dimensional Spaces}

	In the visual domain there exists an obvious analogy to Borges' Library. His 25-letter alphabet is substituted with the 2-letter alphabet of binary, lines of text becomes rows of pixels, and the page becomes an image. Because this is such a well defined and unambiguous situation, a computer can steadily enumerate every possible combination of pixels in this image.
	
	The best known implementation of this idea is ``Every Icon'' by John F. Simon Jr.\cite{john_f._simon_jr._every_????} He uses a 32x32 grid of black and white pixels, and enumerates through the possibilities in binary. The first row alone has $2^{32}$, or about 4.3 billion, combinations. At 100 frames per second, this takes a little over 16 months to complete. The entire image has $2^{32*32}=2^{1024}$ possibilities, or about $10^{308}$. This will take on the order of $10^{298}$ years to complete. This number is so incredibly large, it's just shy of the largest ``dictionary number'', a ``centillion'', valued at $10^{303}$. In interviews, Simon calls this number ``several hundred trillion''\cite{matthew_mirapaul_in_1997}:
	
	\begin{quote}
	Because there's no word for that amount of time and no word for that large a number, several hundred trillion years is my way of making you think about a very, very long time.
	\end{quote}
	
	If we didn't care about watching ``Every Icon'', and just wanted the computer to consider every possible variation without displaying them, it would still take around $10^{290}$ years. Simon also says:
	
	\begin{quote}
	We could be looking at something that has meaning, but because of who we are and what we are now, we might not recognize it.
	\end{quote}
	
	Which is reminiscent of Borges' Library and its many languages---most of which no one speaks.
	
	There are a number of other works resembling ``Every Icon'' worth mentioning. Two were created in 1996, the same year as ``Every Icon'': one piece by Jim Campbell called ``The End''\cite{jim_campbell_end_1996} and another by Sintron called ``God's Eye''.\cite{sintron_gods_2003}
	
	``The End'' was constructed with custom electronics, and is meant to be displayed on a television rather than in a browser. It begins against a black background, while ``Every Icon'' begins against a white one. This represents the difference between electronics and paper---Simon attributes his white background to his drawing practice, and one can imagine that Campbell's black background is due to the fact that ``false'', ``nothing'', ``zero'',  or``the absence of electricity'' directly implies ``the absence of light''. Campbell's piece also has a higher bit depth while maintaining a similar resolution, making it exponentially more complex with respect to the possible images it may display.
	
	``God's Eye'' runs at 800x600 pixels rather than 32x32, and uses 24-bit color rather than 1-bit black and white. While the resolution of ``Every Icon'' is tied to the standard computer icon resolution, ``God's Eye'' is derived from the screen resolution of the time. To get a vague idea of how complex this image space is, consider that by the time ``Every Icon'' is completed only the first 42 pixels in the first row of ``God's Eye'' will have finished. For this reason, ``God's Eye'' runs in non-real-time, and calculates ``97,000 images per second on a 155MHz computer''. While ``Every Icon'' and ``The End'' are fairly disembodied, ``God's Eye'' is closely tied to a physical installation involving a pedestal for the computer screen in front of a blue velvet couch, surrounded by two prints that are completely black and completely white, with a small amorphous chrome sculpture sitting on the floor. In interviews, Sintron has criticized ``Every Icon'' for being a ``pop piece''\cite{olga_goriunova_and_alexei_shulgin_touching_2003}, and reflects on the audience for enumerative works:
	
	\begin{quote}
	The EI project is a mini version of God's Eye. The EI project was somewhat shortsighted in scope and did not convey the metaphysical aspects of God's Eye but at the same time attracted a larger audience. EI was a pop piece even though it could have been much more. Even with its exposure, I do not think the mass public paid much attention to it beyond a small circle of techno savvy academic individuals. The mass public is not currently capable of understanding these kinds of works. God's Eye was something I felt needed to be out even if misunderstood and unappreciated at the time.
	\end{quote}
	
	Three later enumerative pieces are ``ImageN'' by Leander Seige in 2000\cite{leander_seige_imagen_????}, ``Every Possible Picture Machine'' by Carter Burwell\cite{carter_burwell_every_????} and ``magic mirror'' in 2005 by a user named rom\_min\cite{leonardo_solaas_magic_????}, submitted to a project organized by Leonardo Solaas.
	
	``ImageN'' shares traits with both ``Every Icon'' and ``The End'': it starts with a black background, is meant for the web, and has a low resolution (only up to 150x150). Like ``God's Eye'', it computes images in non-real-time at 20 million per second. A unique characteristic is that it allows you to describe some parameters of the system, such as: how many shades of gray are used, or even whether to use colors, and how large the grid should be. Leander offers a poetic reflection on the project: ``computability is in opposition to the complexity'', affirming that it is very unlikely you'll see anything recognizable.
	
	``Every Possible Picture Machine'' runs at 64x64 pixels using 2-bit grayscale pixels against a black background. Burwell summarizes this whole class of pieces well:
	
	\begin{quote}
	What makes this interesting is that among those pictures will be those of all your ancestors and descendents[sic], the first words of every book that will ever be written. The true digital face of God.

	What makes it less interesting is that it will take a very long time to display all those pictures. But if your computer isn't doing anything else important for the rest of geologic time, do keep it running and you'll see everything.
	\end{quote}
	
	``magic mirror'' was only proposed, never implemented. It runs at 720x576 and 25 frames per second, the standard format for PAL DVDs, using 24-bit color. There is something almost comical about suggesting this format, as it implies that the generated content maybe be delivered on DVDs; and that perhaps this will somehow mitigate the issues associated with the overwhelming amount of content produced.
	
	One final visual enumerative piece is ``De Wensput'' (``The Wishing Well'') by Lars Eijssen and Boele Klopman.\cite{remko_scha_every_2001} Produced in 1991, it precedes ``Every Icon'' by five years. It used a 172x172 pixel grid, and was closely tied to its physical installation: an entire truck bed worth of continuous feed printer paper was driven into the gallery space and fed into the computer at regular intervals. Every time the computer finished rendering a drawing, it would print out the image and move on. Hanging in the gallery space were various representational prints prepared in advance (for example, a portrait of the artists) with a note indicating on what date and time that specific print would appear. It featured an interactive function that allowed people to explore the space while the computer was printing, by recursively zooming in to a time line to select a specific date and time. ``De Wensput'' is unique in that it is the only piece in this category that does not count in binary. Instead, it first considers all the combinations of one black pixel, then two black pixels, then three, etc. Using two levels of ordering (first how many pixels there are, then all the combinations of those pixels), it's almost easier to digest the enumeration. There's something accessible about permutations that is missing from binary enumeration.
	
\subsection{Permutation}

	[The I Ching as a perfect example of a coherent system.]
	
	Standard binary enumeration, the Fu Xi sequence.
	
	Chu Hsi sequence, ordered by bit count. Bears similarity to ``Chord Catalogue'' enumeration. Johnson's enumeration is:
	
	\begin{quote}
	The lowest voice that can rise a half-tone does so and any lower voices descend to their points of departure.
	\end{quote}
	
	While Chu Hsi's might be written:
	
	\begin{quote}
	The highest voice that can rise a half-tone does so and any higher voices descend as low as possible without crossing another.
	\end{quote}
	
	Gray code can either be seen as an enumeration---through an algorithm for constructing an entire sequence---or as an operation to be carried out on a given binary number. The operation consists of a single right shift followed by an XOR against the original value.\footnote{``A Reordering of the Hexagrams of the I Ching'' Victor Mair and Stephen McKenna, "Philosophy East and West", Oct. 1979. proposes an alternative ordering of the hexagrams based on Gray code.}
	
	These all have the problem of having small Hamming distances between adjacent codes---which in these cases also corresponds to low frequency of variation in any given bit.
	
	Other I Ching sequences include the classic King Wen sequence (which has many inverted pairs but is otherwise not mathematically well-defined), The Mawangdui sequence (similar to concatenation of the x/y position of a hexagram).
	
	`allRGB'' by Alexander Christiaan Jacob\cite{alexander_christiaan_jacob_allrgb_2008} is a web-based project that invites visiting programmers to create a square image that contains exactly one instance of the 16 million 24-bit RGB colors. The first submissions, from the author himself, explore basic geometric patterns that correspond to easily programmable algorithms. The tenth image is the first representational image, consisting of 16x16 dithered copies of a portrait. A number of user submitted images have appeared since a reddit post\footnote{\url{http://www.reddit.com/r/programming/comments/b002r/the\_objective\_of\_allrgb\_is\_simple\_to\_create/}} popularized the site. Some of the more difficult to generate images are accompanied by mathematical papers explaining the various techniques involved.\footnote{\url{http://d.hatena.ne.jp/ita/20100222/p1}}
		
	[Changing ringing/method ringing should be discussed here.]
		
\section{Long Pieces}

	Summary: Another way of revealing a space is to examine it on a different scale.

	[Uses existing material.] Paul Slocum's ``Pi House Generator''\cite{paul_slocum_pi_2007} uses the digits of $\pi$ to generate an infinitely long piece of house music. He claims that there is no repetition, but this is technically impossible as the program is deterministic and runs on a computer with limited resources. Furthermore, $\pi$ itself contains an infinite amount of repetition. It would be more accurate to say that the program generates pseudo-random music informed by the digits of $\pi$, and that it's hard to predict what you'll hear next.

	[Uses existing material.] Brian Whitman's ``Eigenradio''\cite{brian_whitman_eigenradio_2005} project listened to multiple radio streams and used principle components analysis to create a single stream that represented all of them simultaneously. Whitman is heavily involved with music analysis, and acknowledges that the only way for a computer to ``understand'' music is by learning its relationship to a large amount of extramusical information:
	
	\begin{quote}
	In the real world, does anyone ever hear music with absolutely no context? Even if you hear something on the radio by accident, you make an assumption about the artist simply because they have music on the radio. You also know the station and the time of day, maybe the song that came after it.
	\end{quote}
	
	[Uses existing material.] Leif Inge's ``9 Beet Stretch''\footnote{\url{http://www.harsmedia.com/SoundBlog/Archief/00550.php}} takes Beethoven's ninth symphony, usually 65 to 75 minutes long, and stretches it to 24 hours with pitch correction. This is a simple act of ``slowing down''; the poetics of ``Longplayer'' without the intense compositional act.
	
	``Longplayer'' by Jem Finer\cite{jem_finer_longplayer_????} is a thousand year long composition scheduled to end on December 31 2999. Composed as the end of the twentieth century, Longplayer is primarily concerned with time and only secondarily with music. The way it solves the musical ``problem'' of creating a one thousand year long composition is interesting: starting with a single 20'20'' composition for Tibetan singing bowls, six two-minute segments are chosen as source material, these segments are played out of phase, and the phase slowly drifts over time. The slowest drifting phase moves with the composition, only completing its phrase when the entire composition is complete. The fastest changing phase makes a cycle every 3.7 days. Longplayer is regularly performed at Trinity Buoy Wharf in London, and is constantly streaming online.
		
	John Cage's ``Organ$^2$/ASLAP'' (As Slow As Possible)\cite{john_cage_as_????} is one of the more infamous performances of a Cage piece. While the first performance lasted only 29 minutes, it is now being performed on a custom built organ at St. Burchardi church in Halberstadt, Germany. Starting with a pause in 2000, it is scheduled to last 639 years, ending in 2640. This composition is likely related to Cage's comment that he was ``trying to find a way to make music that does not depend on time''.
				
\chapter{Only Everything Lasts Forever}

\section{Early Work}

	After discovering so many variations on the ``Every Icon'' theme, I proposed a new work, ``Every Every Icon'':
	
	\begin{quote}
	This work will include all of the above works, as well as any other variations that may be dreamed up in the future. It will accomplish this by iterating through every possible resolution, at every possible framerate, for every possible bit depth, in every possible order.\footnote{\url{http://erraticsemaphore.blogspot.com/2009/08/every-every-icon.html}}
	\end{quote}
	
	Around the same I also created ``Every Song''\footnote{\url{http://openprocessing.org/visuals/?visualID=1170}}, which iterates through all possible equal-tempered songs using twelve notes in the octave. It does this as quickly as possible, with the reference for the tempo coming from the frequency of the lowest note being played. It uses simple binary enumeration, and proceeds in a manner very similar to ``Every Icon'' with the exception of being infinitely long: after the first chord is enumerated, it follows with all the two-chord sequences, then three, etc. I have never personally seen it get beyond playing an F in the second chord.

	``pppd''\cite{kyle_mcdonald_pppd_2009} is an indirect approach to enumeration via a random walk. It draws on computability theory\cite{boolos_computability_2002} and the idea of a Turing-complete system. Turing-completeness refers to the ability of a system to compute anything that can be computed. pppd uses a very simple esoteric programming language known as P'' (``P prime prime'') to accomplish this. While practical programming languages are full of statements like \verb!z = x + y!, \verb!println(z)!, and \verb!for(int i = 0; i < n; i++)!, P'' breaks computing down to its essentials: moving a pointer through memory, changing memory, and looping---requiring only six commands, sometimes written as \verb![, ], +, -, <! and \verb!>!. Because P'' is Turing-complete, any program you could write with a language like C++ or Java could also be accomplished with P'' (albeit in a significantly slower and heavily obfuscated manner). Because any possible program can be represented, this means that any possible digital image or sound can also be written to the program's memory space. pppd generates random code, fitting the minor syntactical requirement of balanced brackets, and then visualizes and sonifies the memory space while running the code. In theory, not only can every image and sound be composed, but every compressed representation as well---along with the decompression algorithm. In practice, pppd produces very regular sequences of flashing colors and glitchy sounds. This acts a revelation of the nature of P'': it's likes to representing high-contrast memory sequences within constrained memory regions. Using another language would generate very different results.
	
	My nandhopper project\cite{kyle_mcdonald_nandhopper_2008} also draws on enumeration via directed random walks. The NAND is a binary logical operation, meaning it takes two inputs that are either true or false and returns a result of true or false. The NAND has the unusual property of being able to represent any possible logical operation on any number of inputs. In other words, while you might normally express a logical statement using multiple operators like IF, AND, OR, and NOT, they can all be reduced to statements that only use NAND. Practically, this means that any possible digital sound synthesis circuit can be represented using only NAND gates.
	
	I first came across NAND gates when exploring capacitive sensing. NAND gates are useful for constructing feedback loops that vary their frequency with respect to capacitance to ground.\footnote{Using a Schmitt trigger NAND like the 4093 you can use one input as an on-off switch while tying the output to the other input via a resistor. The feedback input is also tied to ground via a capacitor. This is a very basic RC circuit.} When I realized that NANDs could be used to simulate every other chip, I started exploring various NAND configurations and ways of connecting multiple NAND circuits. I developed three different configurations for an experimental filmmaker\footnote{Mexican filmmaker Diego Delmar, alias ``Macushi'' \url{http://www.knock.com.mx/}} based on intuitive explorations of these configurations. I published these designs along with detailed instructions on the tutorial-sharing site Instructables.com. In Spring of 2009 I developed a close connection to a complex variation on the NAND-synth that was developed for a performance context, integrating capacitive, resistive, and illumination sensing in a massively entangled feedback circuit. I see these as the first steps in a more general exploration of NAND-synthesis. I imagine a massive matrix of reconfigurable NAND circuits that can be rearranged during performance: constantly responding the the changing flow of logic, able to simulate any imaginable digital synthesizer in a very practical sense, complex enough to be interesting, but deterministic enough to be learned.
	
	Another project, ``Future Fragments'', was an exploration of glitch-collaboration similar to ``The Ceibas Cycle''. I described it as:
	
	\begin{quote}
	An anti-time-capsule: quotes from seven fellow art students, transcribed phonetically and encoded as colors. Prints of these colors were carried by the artists for a summer. Five returned. Two extra prints were accidentally intercepted and also returned. Decoded back into phonemes and re-formed into words, each text offers an indirect account of their respective journeys.\footnote{\url{http://www.flickr.com/photos/kylemcdonald/sets/72157608915887288/}}
	\end{quote}
	
	A significant portion of this piece was the time spent developing a meaningful abstract encoding for phonemes. Just as a lossily encoded movie will yield something perceptually similar to the original even in the face of glitch, I developed a lossless encoding that was perceptually informed---in order to degrade in a perceptually relevant way. Bearing a vague resemblance to our mythologies and parables around the world that have gone through steady glitch (telephone/Chinese whispers and ''I am Sitting in a Room'') and co-evolved to reflect the structure of our collective mind rather than any specific historical event.

\section{The History of MP3}
``Understanding MP3'' introduces perceptual coding with a discussion of metamers, which is a great way of describing reduced representations informed by human perception.

Lossy audio compression taking digital information that describes an audio signal and representing it with less information without producing a significantly different sound. This is called ``perceptual encoding''.\cite{Ruckert05}

Karlheinz Brandenburg's PhD dissertation formed the foundation for the MP3 format. His advisor was interested in transmitting music over telephone lines(ISDN), but the patent office said it was impossible. MP3 was 20 years in development. When asked early on ``What will happen to this?'' he replied ``It could end up in the libraries, like so many other theses, or it could be an international standard''. He adds: ``I didn't dream of hundreds of millions of people''. Fraunhofer knew they wanted to make MP3 ``the'' internet format, and took that opportunity, but didn't realize the full extent of what that meant. Now he's looking into WFS and better DRM solutions.\cite{brandenburg_interviews_2004}
	
Brandenburg admits that there are ``compromises'' in MP3, due to needing to inherit the format of MP2. ``Dry percussive material'' doesn't translate well (pre-echo effect most obvious when listening to castanets, which is not a problem with AAC). Low bitrates have ``bandpass problems at high frequencies'' which he vocally imitates as ``shw-shw-shw''. As a trained listener, he could hear the difference at 192 kbps in a blind test, but can't anymore (``I'm too old''). AAC is what MP3 ``should have been''. MPEG (motion picture experts group) was formed to put video on CD-ROMs, when at the time you had only audio on CD-ROMs (massive compression problem). Layer 2 and Layer 3 were two competing proposals during the MPEG audio format ``shootout'', and were decided to be combined. First PC-based decoders were in 1995. July 14th, 1995 they decided on the file extension ``.mp3'' For a psychoacoustic model he mentions frequency quantization and masking (In \cite{karlheinz_brandenburg_mp3_1999} he describes three ``common types of artifacts'': loss of bandwidth, pre-echoes, and roughness or double-speak.) For masking he gives the example of a train arriving at a station drowning out the conversations around you.

He says how, if you look at the data, you see runs of zeros or ones (that might be compressed by lossless compression routines like RLE), and that ``it's really a miracle with how that is reconstructed to get you the music again'' he says with a smile. He localizes different bitrates to different situations: ``128 might be good enough to listen to it on the airplane or the train''. The way the format is structured, the difference between 128 and 192 is much bigger than 256 and 320: after a certain point it takes more work to represent finer details (in \cite[9]{karlheinz_brandenburg_mp3_1999} he calls it the ``sweet spot''). This might be very loosely analogous to music creation: more people than ever are creating music right now and sharing it, perhaps exponentially, but innovation continues at a steady rate. He smiles when he says that vinyl records have artifacts that ``people come to like''. He talks about Gestalt, and how what we hear is ``clearly influenced from what we expect to hear''. Mentions how recordings can be constructed to fool people who would be able to identify their origin. People buying nice cables because they believe the sound quality will improve. He doesn't dismiss it, he says ``it's real, they think it's better, if they're happy with it then I'm fine as well''. He mentions that he knows people who have many TB of MP3s: ``which means years of uninterrupted listening, which means they have no idea what they have''. (For reference, 1 year at 256 kbps is 7.5 TB) \cite{tom_merritt_real_2010}

While PCM encoding tried to replicate the physicality of audio, as represented on vinyl and tape, MP3 reduces the information to a human-oriented format that makes an attempt at universality. In a way, it is a step towards a universal music.

Brandenburg states that we have all become ``expert listeners'' due to the simple fact that we are well trained at listening to compressed audio.\cite[9]{karlheinz_brandenburg_mp3_1999}

Brandenburg recommends limiting the response of an MP3 or AAC encoder to 16 kHz as there ``are some hints'' that there exist listeners who can identify the difference between complex signals above 16 kHz, but ``the full scientific proof has not yet been given''.\cite[10]{karlheinz_brandenburg_mp3_1999} This serves as a piece of the MP3-ideology, acts as an ontology of sound, and limits musical practice.
	
\section{The General Structure of MP3}
	In order to iterate through the space of MP3, it's essential to understand how the format is structured.
	
	An MP3 file consists of a bit stream that is divided into metadata\footnote{Only the audio data is defined by the ISO standard, which has lead to the proliferation of metadata formats for MP3. This is itself an interesting space, with special limitations and biases. In ID3---the most common metadata format---one byte is alloted for genre, indexing a list of 148 genres. The list includes thirteen varieties of ``Rock'', from ``Southern Rock'' to ``Symphonic Rock'' and ``Gothic Rock'', without a mention of Indian classical music, gamelan, any kind of chant, klezmer, Afro-cuban music, or baroque. Biases can be found in any ontological system, a classic example being the Dewey Decimal System's bias for Christian literature.} and audio data. The audio data is a sequence of logically independent frames physically embedded in two interleaved bitstreams: one stream occurs at regular intervals, the other fills the space in between. The regular data begins with an easily identifiable signature: a sequence of twelve binary ones, or \verb!0xfff!. This sync is part of a 4 byte header that describes the high level characteristics of the current frame, such as the number of channels\footnote{MP3 includes mono and three types of stereo encoding. Any quantitative descriptions of the MP3 format given here are in reference to the mono mode, as OELF is a mono composition.}, samplerate, bitrate, and format version.
	
	Following the header is 17 bytes of ``side information'' (side info). The header and side information together make up the regular bitstream, while the ``main data'' falls in between. The main data is Huffman coded,\footnote{Huffman coding is an algorithm for compressing information. It is based on the principle of replacing more commonly occurring symbol sequences with short codes, and less frequent symbol sequences with longer codes. MP3 uses a modified version of Huffman coding that allows significant variation in the length of the symbol sequences.} and the side info provides a variety of details about how to go about decoding it. Once decoded from its Huffman representation, the main data goes through a series of complex transformations informed by various settings and scalings described in the side info.
	
	Every frame---header, side info, and main data---contains two ``granules'' that describe two consecutive chunks of audio. Sometimes granules share some of their 21 scale factors (a sort of EQ), which is why they are stored within a single frame. Each granule stores 576 ``frequency bands'', decoded into 576 PCM samples, meaning every frame consists of 1152 PCM samples. To understand where these frequency bands come from, it may be easiest to explain them in terms of encoding than decoding. The 576 PCM samples for a single granule are first subdivided into 18 sections consisting of 32 samples each. Each block is then encoded using 32 subband filters resembling critical bands. Finally, each subband block (an 18-element time-varying sequence) is processed using a modified discrete cosine transformation that describes these time-varying sequences with respect to their regular variations. In short, the main data stored in an MP3 is a heavily abstracted representation of the PCM data. It assumes that our understanding of sound is best represented by regularly varying critical bands rather than as a time-varying sequence of variations in relative air pressure.\footnote{There is a lot of detail left out here involving quantization, pre-emphasis, short versus long blocks, global gain, windowing, overlap-add, and scale factors. All these things play a significant role in the sound of MP3, but the inverse modified discrete cosine transformation and subband synthesis are at the heart of the algorithm.}
	
\section{How Many Sound Can We Hear?}

	With the structure of MP3 in mind, under the assumption that MP3 represents all the sounds we can distinguish, we can make an approximation as to how many sounds that is. If the fundamental unit of the MP3 is the frame, and we are working at 128 kbits, every frame can have an average length of 418 bytes, or 3336 bits. In mono, 17 of these bytes are dedicated to the side information and 4 are the header. Most of the variation occurs in the main data, which is 397 remaining bytes. Therefore the total variation is $2^{397*8}=2^{3176}=10^{956}$. The number of 26 ms sounds we can distinguish is on the order of 10 followed by 956 zeros. If the fundamental unit is considered instead to be the granule, this cuts the power in half down to $10^{478}$. Considering AAC also further reduces the necessary bitrate by half, I'll place an upper bound on the number of uniquely distinguishable ``sound-atoms'' we can distinguish at $10^{239}$. The number of these sounds that can be played in a year is on the order of $10^9$, meaning that, played in succession, it would take $10^{230}$ years.
	
	MP3's 26 ms is approximately in line with Bob Snyder's analysis of musical meaning and memory. He groups sonic memory into three categories: event fusion, melodic and rhythmic grouping, and form. The cutoffs for each are between 1/32 second and 1/16 second, and 8 seconds and 16 seconds respectively.\footnote{p 12, ``Music and Memory''}
	
I haven't spent enough time yet with Herbert Bruen's ``When Music Resists Meaning''.\cite{Bruen04}
	
	Another perspective on musical meaning: in ``Music and Discourse'' by Jean-Jacques Nattiez\cite{nattiez_music_1990}, he describes musical meaning as a ``symbolic web''. He essentially accepts a coherentist account of meaning, saying it emerges ``when an object is placed in relationship to a horizon''. His view of this horizon is informed by psychologist Robert Franc\`es, who outlines four types of judgments:
	
\begin{enumerate}
	\item Normative judgments (such as personal taste)
	\item Objective/technical judgments (timbre, tempo and genre)
	\item Judgments about meaning (extramusical references)
		\begin{enumerate}
			\item an individual referent (connection to personal experience)
			\item concrete meaning (naturally occurring sounds)
			\item abstract meaning (playfulness, serenity or hierarchy)
		\end{enumerate}
	\item ``Affirmations of interior order'' (psychological effects upon the listener)
\end{enumerate}

	Of these relationships, a number are independent of judgment: ``concrete meaning'', and arguably ``technical'' meaning as well. These relationships exist regardless of whether a conscious observer makes a judgment about them. The ``objective/technical'' relationships are suspicious only in so far as properties such as tempo and vibrato are constructs of a cultural tradition, but other properties such as relative amplitude, frequency, and timbre are more physically based rather than culturally.
	
\section{Structurally Informed Enumeration}

	From the above section it should be apparent that MP3 is not a simple format. At least, not in the way a bitmap image or PCM file is ``simple''. To iterate through possible bitmap images, it is only necessary to pick your dimensions, bit depth, and number of channels---from there you can simply increment a binary number of equivalent length. Likewise with PCM: it is only necessary to pick your file length, bit depth, and number of channels. Unlike these formats, MP3 has a strict ``syntax'' that must be followed. Although the bitstream may appear to be a noisy sequence of ones and zeros, not just any noise is allowed.
	
	Enumerating PCM or bitmap files might be akin to the placing a monkey at a typewriter: from a fixed set of symbols, they can always pick any of the symbols. Enumerating MP3 frames is more similar to placing a human at the typewriter, and telling them they are allowed to type anything that conforms to basic rules of spelling and grammar. If conducted in Japanese, the resulting string of symbols would appear noisy to me---but to someone who understands Japanese there is a deep structure. Likewise, the hexadecimal sequence:
	
	\begin{verbatim}
	...ff fb a2 00 ea 84 43 dc 64 56 51 ec 1d 22 7b 8c 8a ca 3d 83 a4...
	\end{verbatim}
	
	Will appear noisy to the untrained eye, but to someone who understands MP3 it is obvious that this describes the header and the beginning of the side info for a stereo MP3 encoded at 160 kbps and 44.1 kHz.\footnote{Specifically, a frame from the middle of ``Cajun Waltz''; track 12 off Alan Lomax's 1934 ``Cajun and Creole Music''.}
	
	In the same way you might enumerate possible English sentences by first describing a hierarchical grammar and then providing a vocabulary, I will describe a method for enumerating MP3 frames that first accounts for their syntax, followed by the content that may be placed within that structure.
	
\section{Shuffle Enumeration}

	Summary: discuss the insufficiency of traditional enumerations at exploring a large space. Describe the shuffle enumeration technique developed for this project, and how the resulting bits were rearranged as the final compositional step.
	
\section{Difficulties of Composing for a Large Space}

	(Mention the ISO part 4 compliance testing files as examples of enumeration for technical purposes, as a comparison to this composition.)
	
	(Discuss the notion of ``composing for psychoacoustic entropy''.)
	
	(Organizing everything as tessellation: whenever you take something away from one area, you have to add it to the opposite side. Similar to bit swapping. You must necessarily compose for the entire space simultaneously.)
	
	A basic enumeration of MP3 might be compared to writing out a chromatic scale. You probably wouldn't call a chromatic scale a ``composition'', but you might call one of Shch\"onberg's 12-tone pieces a ``composition''. Once the space is enumerated, it is still waiting to be arranged in an interesting way----simply enumerating the space doesn't justify the work.

	The protagonist of Borges' ``The Aleph'' makes an interesting comment about the difference between how a work is received and how it is justified:
	
	\begin{quote}
	...Daneri's real work lay not in the poetry but in his invention of reasons why the poetry should be admired. Of course, this second phase of his effort modified the writing in his eyes, though not in the eyes of others.
	\end{quote}
	
	Perhaps this is also what John F. Simon Jr. was referring to when he wrote:
	
	\begin{quote}
	While Every Icon is resolved conceptually, it is unresolvable in practice. In some ways the theoretical possibilities outdistance the time scales of both evolution and imagination. It posits a representational system where computational promise is intricately linked to extraordinary duration and momentary sensation.\cite{john_f._simon_jr._given:32_1997}
	\end{quote}
	
\chapter{Conclusion}
	(This chapter will provide a brief summary of everything that was discussed.)

\specialhead{References}
\begin{singlespace}
\bibliographystyle{plain}
\bibliography{original,spiritofnoise,everythingart,glitchart,oelf}
\end{singlespace}

\appendix 
\addtocontents{toc}{\parindent0pt\vskip12pt Appendices}

\chapter{The MP3 Bitstream}
	
	Within the header a few sections must be considered:
	
\begin{itemize}
	\item Layer: Layer III is ``.mp3'', layers I and II are rarely used.
	\item Bitrate: In Layer III, this varies from 32 to 320 kbps.
	\item Samplerate: 32 kHz, 48 kHz, or 44.1 kHz.
	\item Mode: Specifies mono or a variety of stereo.
	\item Copyright: Whether the file is copyrighted.
	\item Original: Whether the file is original, or a copy.\footnote{For obvious reasons, the copyright and original bits are not generally used---despite the best intentions of the ISO.}	
\end{itemize}

	In a normal MP3, most of these fields will remain constant from frame to frame. OELF does not enumerate these variations, and instead chooses a single format: Layer III, 128 kbps, 44.1 kHz, mono, non-copyrighted, and original.
	
	The side info contains:
	
\begin{itemize}
	\item A description of where the main data is located in the interleaved bitstream.
	\item Scale factor sharing information: if the two granules in this frame have similar frequency-domain characteristics, they can share any of four regions of 21 scale factors.
	\item Side info for each granule
\end{itemize}

	The per-granule side info is where most of the variation exists:
	
\begin{itemize}
	\item Size of main data: describes how many bits are encoded in the main data, where the scale factors and Huffman coded data is stored.
	\item Big values: describes what portion of the main data frequency bands are ``big values''---the lower frequency components that tend to have large amplitudes. Following the big values are the ``small values'', up to the end of the main data---higher frequency components with small amplitudes.
	\item Global gain: a straightforward scaling factor applied to the entire granule.
	\item Scale factor bit allocation: an index to a table that describes how much detail the different scale factor bands are coded with.
	\item Window switching flag: Distinguishes between ``normal'' and ``special'' blocks. Normal blocks scale the entire granule with up to 21 scale factors, certain types of special blocks are divided into three short bursts with separate scale factors.
	\item A section that varies depending on the window switching flag value
	\item Pre-emphasis flag: a small concession to high frequency content, this binary flag provides a minor amplification to higher frequency components.
	\item Scale factor quantization: effectively, a multiplier flag that acts on all the scale factors.
	\item Small value table selection: specifies one of two Huffman tables to use for decoding the small values.
\end{itemize}

	There are two forms the section that varies with respect to the window switching flag can take. If the flag is false, and we have a normal block:
	
\begin{itemize}
	\item Huffman table selection: 15 bits
	\item Description of region separation: 7 bits
\end{itemize}

	If the flag is true, we have a special block:
	
\begin{itemize}
	\item Block type: 2 bits
	\item Mixed block flag: 1 bit
	\item Huffman table selection: 10 bits
	\item Subblock gain: 9 bits
\end{itemize}

	Finally, the main data consists of:
	
\begin{itemize}
	\item Scale factor bands
	\item Big valued frequency bands
	\item Small valued frequency bands
\end{itemize}

	The main data is not described in terms of bits as the bit length can vary depending on the side info specifications.

	Some features vary freely, without further constraining other features or being constrained by previous features. For example:

\begin{itemize}
	\item Global gain: 8 bits
	\item Window switching flag: 1 bit
	\item Pre-emphasis flag: 1 bit
	\item Scale factor quantization: 1 bit
	\item Small value table selection: 1 bit
\end{itemize}

	That's $8+1+1+1+1=12$ bits of variation that could be assigned randomly for any given frame without destroying the syntax.\footnote{One of my first experiments with MP3 glitching was to use these 12 bits in a composition titled ``1241385461464'', a glitched version of a recording of 4'33'' taken from YouTube.}
	
	Then we have to decide on the window switching flags for each granule, and the block types. This in turn will constrain the scale factor sharing. This constrains the scale factor bit allocation, which has an influence on the main data size (and in turn influences the big values):
	
\begin{itemize}
	\item Size of main data: 12 bits
	\item Big values: 9 bits
\end{itemize}

	The big values must be smaller than the main data. This leads to $\sum_{i=0}^{2^{12}} min(i,2^9)$ possible combinations.\footnote{In practice, this number must be significantly smaller due to the limited space provided by the combination of the bit reservoir and the bitstream following the side info.}
	
	One piece remains: the main data location. Because the main data location is solely physical (bitstream-oriented) rather than logical (sound-descriptive), it can be derived rather than enumerated.

\end{document}
